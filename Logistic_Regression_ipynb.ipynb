{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlIxvBnqldWD"
      },
      "source": [
        "                                                                     Theoretical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JacXVKfQlS8y"
      },
      "source": [
        "Q(1)What is Logistic Regression, and how does it differ from Linear Regression.?\n",
        "- Logistic Regression and Linear Regression are both supervised learning algorithms used for prediction, but they solve different kinds of problems.\n",
        "\n",
        "\n",
        "Feature\tLinear Regression\tLogistic Regression\n",
        "Purpose\tPredict continuous outcomes (e.g., price, weight)\tPredict categorical outcomes (e.g., yes/no, 0/1)\n",
        "Output\tAny real number\tA probability (between 0 and 1), often turned into a class label (0 or 1)\n",
        "Equation form\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...\n",
        "logit\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "logit(p)=log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...\n",
        "Function used\tDirect line equation (straight line)\tSigmoid function (S-shaped curve)\n",
        "Application Example\tPredicting house prices\tPredicting if an email is spam (yes/no)\n",
        "Some intuition:\n",
        "Linear Regression:\n",
        "\n",
        "You’re fitting a straight line (or hyperplane for multiple variables) to predict a number.\n",
        "\n",
        "It doesn’t care if the output is 15, -3, or 1000.\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "You’re fitting a curve (the sigmoid function) to squash the output between 0 and 1.\n",
        "\n",
        "The output is interpreted as the probability of belonging to a class.\n",
        "\n",
        "Closer to 0 → class 0.\n",
        "\n",
        "Closer to 1 → class 1.\n",
        "\n",
        "Example:\n",
        "- Suppose you have a model that looks at hours studied and predicts:\n",
        "\n",
        "- Linear Regression would predict, say, a score like 78.5.\n",
        "\n",
        "- Logistic Regression would predict, say, a 0.85 probability that the student passes (where pass = 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-mwv_VbFlur"
      },
      "source": [
        "Q(2)What is the mathematical equation of Logistic Regression?\n",
        "- The core equation of logistic regression is based on the logit function, which is the log of the odds of the outcome.\n",
        "\n",
        "The model tries to predict the probability\n",
        "𝑝\n",
        "p that the output is 1 (success/class 1) given some inputs\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,...,x\n",
        "n\n",
        "​\n",
        " .\n",
        "\n",
        "Logistic Regression model:\n",
        "\n",
        "𝑝\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "p=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑝\n",
        "p = predicted probability that\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  = intercept\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,...,β\n",
        "n\n",
        "​\n",
        "  = coefficients for each feature\n",
        "\n",
        "𝑒\n",
        "e = Euler’s number (about 2.718)\n",
        "\n",
        "Alternate form (log-odds form):\n",
        "\n",
        "Take the ratio\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "1−p\n",
        "p\n",
        "​\n",
        "  — this is called the odds.\n",
        "\n",
        "Then take the natural logarithm (logit) of the odds:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "So log(odds) is a linear function of inputs.\n",
        "\n",
        "Quick summary:\n",
        "The logit (log-odds) is linear in the inputs.\n",
        "\n",
        "The probability\n",
        "𝑝\n",
        "p is a nonlinear transformation (via the sigmoid function) of that linear model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kw-bge7F0U7"
      },
      "source": [
        "Q(3) Why do we use the Sigmoid function in Logistic Regression?\n",
        "- Love this question — because the \"why\" behind sigmoid is super important to really get logistic regression.\n",
        "\n",
        "Here's the simple answer:\n",
        "\n",
        "We use the sigmoid function in Logistic Regression to map any real-valued number to a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "The sigmoid function is:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " .\n",
        "\n",
        "Why specifically the Sigmoid?\n",
        "Bounded Output:\n",
        "\n",
        "Linear functions can produce anything from\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞.\n",
        "\n",
        "But probabilities must be between 0 and 1.\n",
        "\n",
        "Sigmoid ensures the output is always between (0, 1).\n",
        "\n",
        "Smooth Transition:\n",
        "\n",
        "Sigmoid smoothly transitions from 0 to 1.\n",
        "\n",
        "Small changes in input near 0 (the center) cause significant changes in output — making it sensitive around decision boundaries (good for classification).\n",
        "\n",
        "Interpretability:\n",
        "\n",
        "The output of the sigmoid directly gives you a probability: e.g., 0.85 means 85% chance the label is 1.\n",
        "\n",
        "Differentiability:\n",
        "\n",
        "The sigmoid function is smooth and differentiable, which is necessary for optimization algorithms like gradient descent to work efficiently.\n",
        "\n",
        "Visual intuition:\n",
        "For very large negative\n",
        "𝑧\n",
        "z, sigmoid output is close to 0.\n",
        "\n",
        "For very large positive\n",
        "𝑧\n",
        "z, sigmoid output is close to 1.\n",
        "\n",
        "For\n",
        "𝑧\n",
        "=\n",
        "0\n",
        "z=0, sigmoid output is exactly 0.5.\n",
        "\n",
        "It's S-shaped (\"sigmoid\" actually means \"S-shaped\") and it looks like a soft \"threshold\" around 0.\n",
        "\n",
        "Tiny bonus fact:\n",
        "In some cases, other functions like the softmax (for multiple classes) or tanh (output between -1 and 1) are used — but for binary classification, sigmoid is perfect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GB28ncZGU2L"
      },
      "source": [
        "Q(4)What is the cost function of Logistic Regression?\n",
        "- Alright, let’s get into it — the cost function in Logistic Regression is super important because it’s how the model learns the best parameters.\n",
        "\n",
        "First, why not just use Mean Squared Error (MSE)?\n",
        "In Linear Regression, we use MSE (mean squared error).\n",
        "\n",
        "But if you use MSE in Logistic Regression, you end up with a non-convex cost function (wavy, lots of local minima) — bad for optimization.\n",
        "\n",
        "So instead, we use a different loss called Log Loss or Binary Cross-Entropy Loss, which is convex and nicely shaped for optimization (a single global minimum).\n",
        "\n",
        "The Cost Function (for one training example):\n",
        "For an input\n",
        "𝑥\n",
        "x with label\n",
        "𝑦\n",
        "y and predicted probability\n",
        "𝑝\n",
        "p:\n",
        "\n",
        "Cost\n",
        "(\n",
        "𝑝\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "−\n",
        "(\n",
        "𝑦\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        ")\n",
        "Cost(p,y)=−(ylog(p)+(1−y)log(1−p))\n",
        "where:\n",
        "\n",
        "𝑦\n",
        "y is the true label (0 or 1)\n",
        "\n",
        "𝑝\n",
        "p is the predicted probability that\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1\n",
        "\n",
        "Breaking it down:\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1:\n",
        "\n",
        "The cost becomes\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "−log(p)\n",
        "\n",
        "So if\n",
        "𝑝\n",
        "p is close to 1 (correct prediction), cost is low; if\n",
        "𝑝\n",
        "p is close to 0 (wrong prediction), cost is high.\n",
        "\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0:\n",
        "\n",
        "The cost becomes\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "−log(1−p)\n",
        "\n",
        "If\n",
        "𝑝\n",
        "p is close to 0 (correct prediction), cost is low; if\n",
        "𝑝\n",
        "p is close to 1 (wrong prediction), cost is high.\n",
        "\n",
        "The overall cost (for the whole dataset) is the average over all examples:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "−\n",
        "(\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "J(θ)=\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " −(y\n",
        "(i)\n",
        " log(p\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−p\n",
        "(i)\n",
        " ))\n",
        "where:\n",
        "\n",
        "𝑚\n",
        "m = number of training examples\n",
        "\n",
        "𝜃\n",
        "θ = parameters/weights\n",
        "\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "p\n",
        "(i)\n",
        "  = predicted probability for example\n",
        "𝑖\n",
        "i\n",
        "\n",
        "- Intuitive meaning:\n",
        "If your model predicts well → low cost.\n",
        "\n",
        "- If your model predicts badly → high cost.\n",
        "\n",
        "During training, we minimize this cost using an optimization algorithm like Gradient Descent.\n",
        "\n",
        "- In short:\n",
        "➡️ Logistic Regression's cost function is based on how wrong the predicted probabilities are, penalizing confident wrong predictions heavily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOo0ktUoGsne"
      },
      "source": [
        "Q(5)What is Regularization in Logistic Regression? Why is it needed?\n",
        "- What is Regularization in Logistic Regression?\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function.\n",
        "\n",
        "In Logistic Regression, regularization modifies the cost function like this:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "original cost\n",
        "+\n",
        "regularization term\n",
        "J(θ)=original cost+regularization term\n",
        "Where the regularization term penalizes large weights (i.e., large\n",
        "𝜃\n",
        "θ values).\n",
        "\n",
        "Types of Regularization commonly used:\n",
        "L2 Regularization (Ridge):\n",
        "\n",
        "Adds the sum of squares of the parameters to the cost.\n",
        "\n",
        "New cost function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [−y\n",
        "(i)\n",
        " log(p\n",
        "(i)\n",
        " )−(1−y\n",
        "(i)\n",
        " )log(1−p\n",
        "(i)\n",
        " )]+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝜆\n",
        "λ = regularization strength (hyperparameter)\n",
        "\n",
        "𝑚\n",
        "m = number of examples\n",
        "\n",
        "𝑛\n",
        "n = number of features\n",
        "\n",
        "𝜃\n",
        "𝑗\n",
        "θ\n",
        "j\n",
        "​\n",
        "  = weight for feature\n",
        "𝑗\n",
        "j\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "Adds the sum of absolute values of the parameters.\n",
        "\n",
        "Promotes sparsity — some weights can become exactly 0 (feature selection effect).\n",
        "\n",
        "Why do we need Regularization?\n",
        "- ✅ To prevent overfitting:\n",
        "\n",
        "Without regularization, the model might fit the noise in the training data — it would perform very well on training data but badly on unseen data.\n",
        "\n",
        "- ✅ To simplify the model:\n",
        "\n",
        "Regularization tends to shrink the coefficients towards zero, leading to simpler, more interpretable models.\n",
        "\n",
        "- ✅ To handle multicollinearity:\n",
        "\n",
        "If features are highly correlated, regularization helps stabilize the model by limiting how much it relies on any one feature.\n",
        "\n",
        "- ✅ To encourage feature selection (especially L1):\n",
        "\n",
        "- L1 regularization can literally set some feature weights to zero → automatic feature selection.\n",
        "\n",
        "- Short version:\n",
        "Without regularization: Model becomes too complex and overfits.\n",
        "\n",
        "With regularization: Model becomes simpler, more robust, and generalizes better.\n",
        "\n",
        "- 🔵 Summary:\n",
        "\n",
        "Regularization is like telling the model:\n",
        "\"Hey, I want you to fit the data well, but not at the cost of becoming unnecessarily complicated.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEuO1HjdHEij"
      },
      "source": [
        "Q(6)Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "- 🟦 1. Ridge Regression (L2 Regularization)\n",
        "Penalty: Adds the squared magnitude of the coefficients (weights) to the cost function.\n",
        "\n",
        "- Cost function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "- Effect:\n",
        "\n",
        "Shrinks coefficients toward zero, but never exactly zero.\n",
        "\n",
        "All features are kept; none are completely eliminated.\n",
        "\n",
        "- Use case:\n",
        "\n",
        "Best when you have many small/medium-sized important features.\n",
        "\n",
        "Good for multicollinearity (when features are correlated).\n",
        "\n",
        "- 🟨 2. Lasso Regression (L1 Regularization)\n",
        "Penalty: Adds the absolute value of the coefficients to the cost function.\n",
        "\n",
        "Cost function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "J(θ)=Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect:\n",
        "\n",
        "Shrinks some coefficients exactly to zero.\n",
        "\n",
        "So, it effectively performs feature selection — keeping only the most important features.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Best when only a few features are actually important (sparse data).\n",
        "\n",
        "- 🟧 3. Elastic Net Regression (Combination of L1 and L2)\n",
        "Penalty: Mixes L1 and L2 regularization.\n",
        "\n",
        "Cost function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=Loss+λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "or, often, a simplified form:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "Loss\n",
        "+\n",
        "𝜆\n",
        "(\n",
        "𝛼\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        ")\n",
        "J(θ)=Loss+λ(α\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣+(1−α)\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        " )\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "∈\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "α∈[0,1] controls the balance between L1 and L2.\n",
        "\n",
        "Effect:\n",
        "\n",
        "Can both select features (like Lasso) and shrink coefficients (like Ridge).\n",
        "\n",
        "- Use case:\n",
        "\n",
        "Best when you have many features, and some are correlated or irrelevant.\n",
        "\n",
        "More flexible than pure L1 or L2.\n",
        "\n",
        "- 🚀 Quick Comparison Table:\n",
        "\n",
        "Feature\tRidge (L2)\tLasso (L1)\tElastic Net (L1 + L2)\n",
        "Shrinks coefficients\t✅\t✅\t✅\n",
        "Sets some coefficients to zero (feature selection)\t❌\t✅\t✅ (depending on\n",
        "𝛼\n",
        "α)\n",
        "Handles multicollinearity\t✅\t❌\t✅\n",
        "Good when\tMany small effects\tFew strong effects\tMix of both\n",
        "Summary in one line:\n",
        "\n",
        "Ridge → Keep everything, shrink them.\n",
        "\n",
        "Lasso → Pick the important ones, kill the rest.\n",
        "\n",
        "Elastic Net → Smart mix — shrink and pick!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTZ04wMoHoaN"
      },
      "source": [
        "Q(7) When should we use Elastic Net instead of Lasso or Ridge?\n",
        "- You should consider Elastic Net when you expect both:\n",
        "\n",
        "some features are strongly correlated (multicollinearity),\n",
        "\n",
        "and you need feature selection (sparse model — setting some coefficients to zero).\n",
        "\n",
        "Quick breakdown:\n",
        "\n",
        "Ridge regression (L2 penalty) → good when many features are correlated and you want to shrink coefficients, but none will become exactly zero. No feature selection.\n",
        "\n",
        "Lasso regression (L1 penalty) → good for feature selection because it can set some coefficients exactly to zero. But it struggles when features are highly correlated — it tends to pick one and ignore the others arbitrarily.\n",
        "\n",
        "Elastic Net (combines L1 + L2) → gives you the best of both:\n",
        "\n",
        "Encourages sparsity (like Lasso),\n",
        "\n",
        "Handles multicollinearity better (like Ridge).\n",
        "\n",
        "The Elastic Net loss function is:\n",
        "\n",
        "Loss\n",
        "=\n",
        "RSS\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=RSS+λ\n",
        "1\n",
        "​\n",
        " ∑∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        " ∑β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "where\n",
        "𝜆\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  controls the Lasso part and\n",
        "𝜆\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        "  controls the Ridge part.\n",
        "\n",
        "- Use Elastic Net when:\n",
        "\n",
        "- You have many features (maybe more features than samples),\n",
        "\n",
        "- You suspect that some features are correlated,\n",
        "\n",
        "- You want automatic feature selection, but don't want to lose groups of correlated features,\n",
        "\n",
        "- You want a model that is a little more stable than Lasso alone.\n",
        "\n",
        "- In practice, Elastic Net often just outperforms pure Lasso or pure Ridge when the problem is complex.\n",
        "\n",
        "- Example: If you're working on gene expression data (thousands of genes, lots of correlation) — Elastic Net would usually outperform plain Lasso or Ridge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ5WfFg0IhW7"
      },
      "source": [
        "Q(8)What is the impact of the regularization parameter (λ) in Logistic Regression?'\n",
        "- Alright — in Logistic Regression, the regularization parameter\n",
        "𝜆\n",
        "λ (or sometimes\n",
        "𝐶\n",
        "C, depending on the library) plays a huge role in controlling the model’s complexity.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "𝜆\n",
        "λ (regularization strength) controls how much you \"punish\" large coefficients.\n",
        "\n",
        "Large\n",
        "𝜆\n",
        "λ → more regularization → smaller coefficients → simpler model.\n",
        "\n",
        "Small\n",
        "𝜆\n",
        "λ → less regularization → model can have large coefficients → more flexible, but riskier (overfitting).\n",
        "\n",
        "Impact on the model:\n",
        "\n",
        "\n",
        "λ\tCoefficients\tModel behavior\n",
        "High λ (strong penalty)\tShrink toward 0\tUnderfit risk, high bias\n",
        "Low λ (weak penalty)\tCan be large\tOverfit risk, high variance\n",
        "Extra point about scikit-learn:\n",
        "In scikit-learn’s LogisticRegression, they actually use\n",
        "𝐶\n",
        "=\n",
        "1\n",
        "𝜆\n",
        "C=\n",
        "λ\n",
        "1\n",
        "​\n",
        " .\n",
        "\n",
        "So small C means strong regularization.\n",
        "\n",
        "Large C means weak regularization.\n",
        "\n",
        "It's a little confusing at first but once you know, you know.\n",
        "\n",
        "Visually, you can think of it like this:\n",
        "\n",
        "High λ: smoother, simpler decision boundaries (but maybe missing the details),\n",
        "\n",
        "Low λ: very wiggly, detailed boundaries (but might be too sensitive to noise).\n",
        "\n",
        "Short summary:\n",
        "\n",
        "Regularization (λ) in Logistic Regression helps balance overfitting vs underfitting by controlling the size of the model coefficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQTdxu4_I1-B"
      },
      "source": [
        "Q(9)What are the key assumptions of Logistic Regression?\n",
        "- Good one — Logistic Regression is pretty flexible, but it does have some important assumptions you should know:\n",
        "\n",
        "- 🔑 Key assumptions:\n",
        "Linear relationship between the features and the log-odds\n",
        "\n",
        "Not between features and output directly.\n",
        "\n",
        "- Logistic Regression models the log-odds (logit) of the probability as a linear combination of the features:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "So it assumes the logit is a linear function of the predictors.\n",
        "\n",
        "- Independence of observations\n",
        "\n",
        "Each data point should be independent.\n",
        "\n",
        "No autocorrelation (important if you work with time series or clustered data).\n",
        "\n",
        "No (or little) multicollinearity among predictors\n",
        "\n",
        "Features shouldn't be highly correlated with each other.\n",
        "\n",
        "If they are, coefficients can become unstable (big swings for tiny changes).\n",
        "\n",
        "- Techniques like Variance Inflation Factor (VIF) can help check for multicollinearity.\n",
        "\n",
        "- Large sample size\n",
        "\n",
        "- Logistic Regression uses maximum likelihood estimation (MLE), which works better with more data.\n",
        "\n",
        "- Small sample sizes can make the estimates biased or not converge well.\n",
        "\n",
        "Correctly specified model\n",
        "\n",
        "All relevant variables are included.\n",
        "\n",
        "- No important variables are missing.\n",
        "\n",
        "No irrelevant variables causing noise (ideally).\n",
        "\n",
        "- (Realistically, we often don't know this for sure, but it's a theoretical assumption.)\n",
        "\n",
        "Binary or categorical dependent variable\n",
        "\n",
        "Standard Logistic Regression assumes a binary outcome (0/1, yes/no, etc.).\n",
        "\n",
        "(Multinomial Logistic Regression is an extension for 3+ categories.)\n",
        "\n",
        "- ✅ Not needed:\n",
        "\n",
        "The predictors do not need to be normally distributed.\n",
        "\n",
        "- Homoscedasticity (constant variance of errors) is not required (unlike in Linear Regression).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-ne7mLJVlt"
      },
      "source": [
        "Q(10)What are some alternatives to Logistic Regression for classification tasks?\n",
        "- 1. Decision Trees\n",
        "How it works: Decision trees split data into subsets based on the most significant feature at each step. It's a non-linear model.\n",
        "\n",
        "Advantages: Simple to understand, easy to visualize, doesn't require feature scaling, and can handle non-linear relationships.\n",
        "\n",
        "Disadvantages: Prone to overfitting, especially on small datasets or noisy data.\n",
        "\n",
        "Use cases: Works well for both categorical and numerical data, especially when you need interpretability.\n",
        "\n",
        "- 2. Random Forest\n",
        "How it works: Random Forest is an ensemble method that creates multiple decision trees and combines their predictions (using majority vote for classification).\n",
        "\n",
        "Advantages: More robust than a single decision tree (reduces overfitting), handles non-linear relationships well, and performs well on a wide range of problems.\n",
        "\n",
        "Disadvantages: Can be computationally expensive, hard to interpret (since it’s a collection of trees).\n",
        "\n",
        "Use cases: Great for structured/tabular data and large datasets.\n",
        "\n",
        "- 3. Support Vector Machines (SVM)\n",
        "How it works: SVM tries to find the hyperplane that best separates the classes. It maximizes the margin between the closest points of each class (called support vectors).\n",
        "\n",
        "Advantages: Effective in high-dimensional spaces, good for complex decision boundaries (non-linear), and can handle binary and multi-class classification.\n",
        "\n",
        "Disadvantages: Computationally expensive, especially with large datasets; performance depends on the right kernel and hyperparameters.\n",
        "\n",
        "Use cases: Effective in high-dimensional data, such as text classification (e.g., using TF-IDF features for text).\n",
        "\n",
        "- 4. k-Nearest Neighbors (k-NN)\n",
        "How it works: Classifies a new point based on the majority class of its k nearest neighbors in the training data (using distance metrics like Euclidean distance).\n",
        "\n",
        "Advantages: Simple to understand, no training phase (lazy learning), naturally handles multi-class classification.\n",
        "\n",
        "Disadvantages: Can be slow for large datasets (as predictions require checking all training points), sensitive to irrelevant features and data scaling.\n",
        "\n",
        "Use cases: Good for small to medium-sized datasets, pattern recognition tasks (like image classification), and real-time prediction.\n",
        "\n",
        "- 5. Gradient Boosting Machines (GBM)\n",
        "How it works: Builds a strong classifier by combining multiple weak models (usually decision trees), each trying to correct the errors of the previous one. Examples include XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "Advantages: Often produces highly accurate models, especially when tuned properly. Can handle non-linear relationships.\n",
        "\n",
        "Disadvantages: Can overfit if not regularized well, computationally intensive, and harder to interpret.\n",
        "\n",
        "Use cases: Works well on structured/tabular data and competitions like Kaggle, especially when accuracy is key.\n",
        "\n",
        "- 6. Naive Bayes\n",
        "How it works: Based on Bayes' theorem and assumes the features are conditionally independent given the class. There are variants like Gaussian Naive Bayes (for continuous data) and Multinomial Naive Bayes (for discrete data).\n",
        "\n",
        "Advantages: Very fast, works well with high-dimensional data (e.g., text classification), and is easy to implement.\n",
        "\n",
        "Disadvantages: The independence assumption often doesn't hold in real-world data (leading to suboptimal performance), not great for complex relationships.\n",
        "\n",
        "Use cases: Text classification (spam filtering, sentiment analysis), document categorization.\n",
        "\n",
        "- 7. Neural Networks (Deep Learning)\n",
        "How it works: Deep learning models consist of multiple layers of neurons (nodes), each layer transforming the input data and passing it to the next layer. These models learn hierarchical features from raw data.\n",
        "\n",
        "Advantages: Highly powerful, can model very complex, non-linear relationships, especially for large datasets.\n",
        "\n",
        "Disadvantages: Requires a lot of data and computational power, hard to interpret (black-box), sensitive to hyperparameters.\n",
        "\n",
        "Use cases: Image classification, speech recognition, natural language processing (e.g., using CNNs or RNNs).\n",
        "\n",
        "- 8. Logistic Regression with Regularization (Elastic Net, Lasso, Ridge)\n",
        "How it works: Regularized logistic regression methods (like Lasso and Ridge) can help handle issues like multicollinearity, overfitting, or variable selection.\n",
        "\n",
        "Advantages: Simpler than some of the complex models above, interpretable coefficients (especially Lasso for feature selection).\n",
        "\n",
        "Disadvantages: Still constrained by the assumptions of linearity in log-odds and may not work as well on complex data.\n",
        "\n",
        "Use cases: When you need a balance between interpretability and performance (such as in business applications with many features).\n",
        "\n",
        "- 9. AdaBoost\n",
        "How it works: AdaBoost (Adaptive Boosting) combines multiple weak classifiers (often decision trees) into one strong classifier by focusing more on the errors made by the previous classifier.\n",
        "\n",
        "Advantages: Can boost the performance of weak classifiers, handles both binary and multi-class classification.\n",
        "\n",
        "Disadvantages: Sensitive to noisy data and outliers, may overfit if the base classifier is too complex.\n",
        "\n",
        "Use cases: Similar to Random Forest, but may perform better on some specific tasks due to boosting.\n",
        "\n",
        "- 10. Multilayer Perceptron (MLP)\n",
        "How it works: A feedforward neural network model that learns from data in layers (hidden layers), transforming the inputs and learning complex relationships.\n",
        "\n",
        "Advantages: Can learn any non-linear function, flexible, powerful for large datasets.\n",
        "\n",
        "Disadvantages: Requires large amounts of data and computing power, difficult to interpret.\n",
        "\n",
        "Use cases: Used in scenarios requiring complex decision boundaries, like image and speech classification.\n",
        "\n",
        "When to choose each:\n",
        "Decision Trees/Random Forests: When you need interpretability and can deal with less complexity.\n",
        "\n",
        "SVM/Gradient Boosting: For more complex, higher-dimensional datasets, especially when accuracy is the goal.\n",
        "\n",
        "Naive Bayes: If you're working with text data or when features are independent.\n",
        "\n",
        "Neural Networks: If you have massive amounts of data (e.g., images, audio, or time series).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiAPC1XHJ1Vq"
      },
      "source": [
        "Q(11)What are Classification Evaluation Metrics?\n",
        "- Classification evaluation metrics are tools used to measure the performance of a classification model, helping you understand how well the model is making predictions. These metrics are crucial when selecting the best model, as different metrics capture different aspects of model performance.\n",
        "\n",
        "Here are the key classification evaluation metrics:\n",
        "\n",
        "- 1. Accuracy\n",
        "Definition: The proportion of correctly classified instances out of the total instances.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "True Positives (TP)\n",
        "+\n",
        "True Negatives (TN)\n",
        "Total instances\n",
        "Accuracy=\n",
        "Total instances\n",
        "True Positives (TP)+True Negatives (TN)\n",
        "​\n",
        "\n",
        "Advantages: Simple and easy to understand.\n",
        "\n",
        "Disadvantages: Can be misleading, especially with imbalanced datasets (e.g., if one class heavily outweighs the other).\n",
        "\n",
        "- 2. Precision\n",
        "Definition: The proportion of true positive predictions among all positive predictions. In other words, how many of the predicted positives are actually positive.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "True Positives (TP)\n",
        "True Positives (TP)\n",
        "+\n",
        "False Positives (FP)\n",
        "Precision=\n",
        "True Positives (TP)+False Positives (FP)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "Advantages: Useful when the cost of false positives is high (e.g., spam detection where you don't want to mistakenly flag legitimate emails).\n",
        "\n",
        "Disadvantages: Doesn't account for false negatives (missed positive cases).\n",
        "\n",
        "- 3. Recall (Sensitivity or True Positive Rate)\n",
        "Definition: The proportion of actual positive instances that were correctly identified by the model. It shows how many of the true positives were caught.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "True Positives (TP)\n",
        "True Positives (TP)\n",
        "+\n",
        "False Negatives (FN)\n",
        "Recall=\n",
        "True Positives (TP)+False Negatives (FN)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "Advantages: Important when the cost of false negatives is high (e.g., in medical diagnoses where missing a positive case is dangerous).\n",
        "\n",
        "Disadvantages: Doesn't account for false positives.\n",
        "\n",
        "- 4. F1-Score\n",
        "Definition: The harmonic mean of precision and recall, which gives a balance between the two. It's especially useful when you need a single metric to compare models, and there's an imbalance between precision and recall.\n",
        "\n",
        "Formula:\n",
        "\n",
        "F1-Score\n",
        "=\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1-Score=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "Advantages: Helps balance precision and recall, making it a good choice when you care about both false positives and false negatives.\n",
        "\n",
        "Disadvantages: It may not give a clear preference when one of the metrics (precision or recall) is much more important than the other.\n",
        "\n",
        "- 5. Specificity (True Negative Rate)\n",
        "Definition: The proportion of actual negative instances that were correctly identified by the model. It tells you how well the model avoids false positives.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Specificity\n",
        "=\n",
        "True Negatives (TN)\n",
        "True Negatives (TN)\n",
        "+\n",
        "False Positives (FP)\n",
        "Specificity=\n",
        "True Negatives (TN)+False Positives (FP)\n",
        "True Negatives (TN)\n",
        "​\n",
        "\n",
        "Advantages: Useful when false positives are costly, like in cases of fraud detection.\n",
        "\n",
        "Disadvantages: Not always as commonly used as precision, recall, or accuracy.\n",
        "\n",
        "- 6. Area Under the Receiver Operating Characteristic Curve (AUC-ROC)\n",
        "Definition: The area under the ROC curve, which plots the True Positive Rate (Recall) against the False Positive Rate. AUC provides a single number to assess the overall performance of the model.\n",
        "\n",
        "Advantages: Evaluates model performance across all classification thresholds and works well with imbalanced data.\n",
        "\n",
        "Disadvantages: May not give a detailed picture for a single threshold value, so additional metrics might be needed.\n",
        "\n",
        "- 7. Receiver Operating Characteristic (ROC) Curve\n",
        "Definition: A graphical representation of the trade-off between True Positive Rate (Recall) and False Positive Rate at various thresholds.\n",
        "\n",
        "Advantages: Helps understand the model's performance over a range of thresholds.\n",
        "\n",
        "Disadvantages: Can be harder to interpret without an AUC score.\n",
        "\n",
        "- 8. Log Loss (Cross-Entropy Loss)\n",
        "Definition: A measure of the model's uncertainty about its predictions. It penalizes incorrect classifications based on how confident the model was about them.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Log Loss\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "Log Loss=−\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(p\n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−p\n",
        "i\n",
        "​\n",
        " )]\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the true class and\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the predicted probability.\n",
        "\n",
        "Advantages: Provides a probabilistic interpretation of the predictions and is especially useful when predicting probabilities rather than hard classes.\n",
        "\n",
        "Disadvantages: More computationally intensive and harder to interpret for non-technical users.\n",
        "\n",
        "- 9. Confusion Matrix\n",
        "Definition: A table that shows the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). It helps to visualize how well the classification model is performing.\n",
        "\n",
        "Advantages: Simple and intuitive. Provides insight into how well the classifier is distinguishing between classes.\n",
        "\n",
        "Disadvantages: Doesn't summarize performance into a single number (though metrics like accuracy and F1 can be derived from it).\n",
        "\n",
        "- 10. Matthews Correlation Coefficient (MCC)\n",
        "Definition: A balanced metric that takes into account True Positives, True Negatives, False Positives, and False Negatives. It is particularly useful for imbalanced datasets.\n",
        "\n",
        "Formula:\n",
        "\n",
        "MCC\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "×\n",
        "𝑇\n",
        "𝑁\n",
        "−\n",
        "𝐹\n",
        "𝑃\n",
        "×\n",
        "𝐹\n",
        "𝑁\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "MCC=\n",
        "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "​\n",
        "\n",
        "TP×TN−FP×FN\n",
        "​\n",
        "\n",
        "- Advantages: Takes all four quadrants of the confusion matrix into account and is particularly good with imbalanced data.\n",
        "\n",
        "Disadvantages: Can be hard to interpret and is less commonly used than other metrics.\n",
        "\n",
        "Summary of When to Use Which Metric:\n",
        "Accuracy: When classes are balanced, and you care about overall performance.\n",
        "\n",
        "Precision: When you care more about minimizing false positives (e.g., fraud detection).\n",
        "\n",
        "Recall: When you care more about minimizing false negatives (e.g., medical diagnoses).\n",
        "\n",
        "F1-Score: When you want to balance precision and recall.\n",
        "\n",
        "AUC-ROC: When you need to evaluate a model across all thresholds.\n",
        "\n",
        "Log Loss: When you need to evaluate the model’s probabilistic confidence.\n",
        "\n",
        "MCC: When working with imbalanced datasets and you want a balanced measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFYxBbS5Kake"
      },
      "source": [
        "Q(11)What are Classification Evaluation Metrics?\n",
        "- Classification evaluation metrics are used to assess the performance of a classification model by comparing its predicted results to the true outcomes. These metrics help you understand how well the model is making predictions, particularly when the data is imbalanced or when you're concerned about false positives and false negatives.\n",
        "\n",
        "Here are the main classification evaluation metrics:\n",
        "\n",
        "- 1. Accuracy\n",
        "Definition: The proportion of correct predictions (both true positives and true negatives) out of the total predictions.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "TP\n",
        "+\n",
        "TN\n",
        "Total instances\n",
        "Accuracy=\n",
        "Total instances\n",
        "TP+TN\n",
        "​\n",
        "\n",
        "TP = True Positives\n",
        "\n",
        "TN = True Negatives\n",
        "\n",
        "FP = False Positives\n",
        "\n",
        "FN = False Negatives\n",
        "\n",
        "Use case: Suitable when classes are balanced.\n",
        "\n",
        "- 2. Precision\n",
        "Definition: The proportion of true positive predictions among all predicted positives. It indicates how many of the positive predictions were actually correct.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "TP\n",
        "TP\n",
        "+\n",
        "FP\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        "\n",
        "Use case: Important when false positives are costly (e.g., in spam detection, where you don't want to flag good emails as spam).\n",
        "\n",
        "- 3. Recall (Sensitivity or True Positive Rate)\n",
        "Definition: The proportion of actual positive instances that were correctly identified by the model. It tells you how many of the actual positives were caught by the model.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "TP\n",
        "TP\n",
        "+\n",
        "FN\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        "\n",
        "Use case: Important when false negatives are costly (e.g., in medical diagnoses, where missing a positive case can be dangerous).\n",
        "\n",
        "- 4. F1-Score\n",
        "Definition: The harmonic mean of precision and recall. It provides a balance between the two metrics and is particularly useful when you need to balance false positives and false negatives.\n",
        "\n",
        "Formula:\n",
        "\n",
        "F1-Score\n",
        "=\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1-Score=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "Use case: Good when there is a class imbalance and you need to balance both false positives and false negatives.\n",
        "\n",
        "- 5. Specificity (True Negative Rate)\n",
        "Definition: The proportion of actual negative instances that were correctly identified by the model. It tells you how well the model avoids false positives.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Specificity\n",
        "=\n",
        "TN\n",
        "TN\n",
        "+\n",
        "FP\n",
        "Specificity=\n",
        "TN+FP\n",
        "TN\n",
        "​\n",
        "\n",
        "Use case: Important when false positives are costly, like in fraud detection.\n",
        "\n",
        "- 6. Area Under the Receiver Operating Characteristic Curve (AUC-ROC)\n",
        "Definition: The area under the ROC curve, which plots the True Positive Rate (Recall) against the False Positive Rate at various thresholds. AUC provides a single number to assess the model's overall performance.\n",
        "\n",
        "Use case: Works well when you need to evaluate model performance across all classification thresholds, especially in imbalanced datasets.\n",
        "\n",
        "- 7. Receiver Operating Characteristic (ROC) Curve\n",
        "Definition: A graphical plot that shows the trade-off between True Positive Rate (Recall) and False Positive Rate at different threshold values.\n",
        "\n",
        "Use case: Useful for evaluating models across various thresholds and comparing models.\n",
        "\n",
        "- 8. Log Loss (Cross-Entropy Loss)\n",
        "Definition: A measure of the uncertainty of the model's predictions. Log loss penalizes the model based on how confident it is about its wrong predictions. The lower the log loss, the better the model's predictions.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Log Loss\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "Log Loss=−\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(p\n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−p\n",
        "i\n",
        "​\n",
        " )]\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the true class and\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the predicted probability.\n",
        "\n",
        "Use case: Best used when you're working with probability outputs (e.g., in probabilistic classifiers) and want to evaluate how well the model predicts probabilities.\n",
        "\n",
        "- 9. Confusion Matrix\n",
        "Definition: A table that shows the number of true positives, true negatives, false positives, and false negatives, helping you visualize how the model is performing.\n",
        "\n",
        "Use case: Provides detailed insight into the classification performance. It is especially useful when dealing with imbalanced datasets.\n",
        "\n",
        "- 10. Matthews Correlation Coefficient (MCC)\n",
        "Definition: A balanced measure that accounts for all four quadrants of the confusion matrix (true positives, true negatives, false positives, false negatives). It's particularly useful for imbalanced datasets.\n",
        "\n",
        "Formula:\n",
        "\n",
        "MCC\n",
        "=\n",
        "TP\n",
        "×\n",
        "TN\n",
        "−\n",
        "FP\n",
        "×\n",
        "FN\n",
        "(\n",
        "TP\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TP\n",
        "+\n",
        "FN\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FN\n",
        ")\n",
        "MCC=\n",
        "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "​\n",
        "\n",
        "TP×TN−FP×FN\n",
        "​\n",
        "\n",
        "Use case: A good metric for imbalanced classification tasks because it provides a more balanced evaluation.\n",
        "\n",
        "Summary:\n",
        "Accuracy: Overall correctness.\n",
        "\n",
        "Precision: Focus on minimizing false positives.\n",
        "\n",
        "Recall: Focus on minimizing false negatives.\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "AUC-ROC: Evaluates performance across all thresholds.\n",
        "\n",
        "Log Loss: Measures the uncertainty of predictions.\n",
        "\n",
        "Confusion Matrix: Visual representation of performance.\n",
        "\n",
        "MCC: Balanced evaluation, especially for imbalanced datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSlYy7DK7W0"
      },
      "source": [
        "Q(12)How does class imbalance affect Logistic Regression?\n",
        "- Class imbalance can significantly impact the performance of Logistic Regression (and many other classification models). When the target classes are imbalanced (i.e., one class is much more frequent than the other), the model tends to be biased towards the majority class. This can lead to poor performance on the minority class, even though the model may show high accuracy overall.\n",
        "\n",
        "- Here’s how class imbalance affects Logistic Regression:\n",
        "\n",
        "- 1. Bias Towards Majority Class\n",
        "Impact: Logistic Regression, like other models, tries to minimize the error. When the classes are imbalanced, it may be more \"cost-effective\" (in terms of error rate) to predict the majority class most of the time.\n",
        "\n",
        "Result: The model could end up predicting the majority class almost all the time, leading to high accuracy but poor recall or precision for the minority class.\n",
        "\n",
        "Example: If you have 95% \"negative\" and 5% \"positive\" instances, the model could classify everything as \"negative\" and still achieve 95% accuracy, despite not predicting a single \"positive\" instance.\n",
        "\n",
        "- 2. Poor Recall for Minority Class\n",
        "Impact: Recall (also known as sensitivity or true positive rate) measures how well the model identifies positive instances (the minority class, in this case). With imbalanced data, the model may focus on the majority class, leading to low recall for the minority class.\n",
        "\n",
        "Result: The model might miss many actual positive instances (false negatives).\n",
        "\n",
        "Example: In fraud detection (where fraudulent transactions are rare), a logistic regression model may fail to detect fraud effectively and predict \"no fraud\" for most cases.\n",
        "\n",
        "- 3. Skewed Decision Boundary\n",
        "Impact: Logistic Regression calculates the log-odds (logistic function) to decide which class to assign. With imbalanced data, the decision boundary may be skewed toward the majority class, because the model is influenced more by the larger class.\n",
        "\n",
        "Result: The model may become less sensitive to the minority class and have an imbalanced decision boundary, making it less likely to classify minority class examples correctly.\n",
        "\n",
        "Example: In a binary classification problem with 95% negatives and 5% positives, the decision boundary may lean heavily towards the negative class, causing a lot of positive class instances to be misclassified as negative.\n",
        "\n",
        "- 4. Overestimating Model Accuracy\n",
        "Impact: Since accuracy is a general measure of correctness (all correct predictions divided by total predictions), it can be misleading when the dataset is imbalanced. The model might achieve high accuracy by simply predicting the majority class most of the time, but this doesn’t reflect how well it performs on the minority class.\n",
        "\n",
        "Result: The model may appear to be performing well on paper, but it is actually failing to predict the minority class.\n",
        "\n",
        "Example: In a dataset where 90% of the data are negative and 10% positive, predicting only the negative class will result in 90% accuracy, even though the model is missing all the positive instances.\n",
        "\n",
        "- 5. Underestimation of the Minority Class\n",
        "Impact: Logistic Regression uses maximum likelihood estimation (MLE), which could favor the majority class during training. This leads to underfitting for the minority class, meaning the model doesn’t learn the minority class patterns well.\n",
        "\n",
        "Result: The minority class may be significantly underrepresented in the model's learned coefficients, leading to poor performance when predicting that class.\n",
        "\n",
        "Example: In rare disease diagnosis, a model trained on imbalanced data may fail to learn key features of the rare disease and be ineffective at detecting it.\n",
        "\n",
        "How to Mitigate the Impact of Class Imbalance in Logistic Regression:\n",
        "Resampling the Dataset\n",
        "\n",
        "Oversampling: Increase the number of minority class samples by duplicating or generating synthetic data (e.g., using SMOTE).\n",
        "\n",
        "Undersampling: Reduce the number of majority class samples by randomly removing instances.\n",
        "\n",
        "Note: Oversampling can help the model learn more about the minority class, while undersampling can speed up training.\n",
        "\n",
        "Adjusting the Decision Threshold\n",
        "\n",
        "Logistic Regression typically uses a default threshold of 0.5 to decide which class to assign. You can lower the threshold for predicting the minority class to improve recall, ensuring that more minority instances are classified as positive.\n",
        "\n",
        "- Example: Instead of predicting class \"1\" for a probability above 0.5, you can lower the threshold to 0.3 to allow more positive predictions.\n",
        "\n",
        "- Using Class Weights\n",
        "\n",
        "Many algorithms, including Logistic Regression, allow you to assign higher weights to the minority class to penalize the misclassification of those instances more heavily. This can help the model focus more on the minority class.\n",
        "\n",
        "- Example: Using the class_weight='balanced' parameter in Logistic Regression (in sklearn) automatically adjusts weights inversely proportional to class frequencies.\n",
        "\n",
        "Use of Evaluation Metrics Other Than Accuracy\n",
        "\n",
        "Instead of relying on accuracy, consider metrics like Precision, Recall, F1-Score, or AUC-ROC to evaluate model performance. These metrics focus more on the model’s performance for both classes, especially the minority class.\n",
        "\n",
        "- Example: Focus on F1-Score to balance precision and recall, or AUC-ROC for evaluating performance across all thresholds.\n",
        "\n",
        "Ensemble Methods\n",
        "\n",
        "- Consider using ensemble models like Random Forest, Gradient Boosting, or AdaBoost that can handle imbalanced data more effectively. These methods build multiple models and aggregate their predictions, often leading to better performance in imbalanced settings.\n",
        "\n",
        "- Conclusion:\n",
        "Class imbalance can lead to biased predictions towards the majority class, low recall for the minority class, and misleading accuracy scores.\n",
        "\n",
        "- To mitigate this, you can resample the data, adjust decision thresholds, use class weights, and focus on more informative evaluation metrics like Precision, Recall, and F1-Score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c82GhbLbDQ"
      },
      "source": [
        "Q(13)What is Hyperparameter Tuning in Logistic Regression?\n",
        "- Hyperparameter tuning in Logistic Regression refers to the process of selecting the best set of hyperparameters for the model to improve its performance. These hyperparameters are set before training and control aspects of the model that affect how it learns from the data. In Logistic Regression, these hyperparameters significantly influence how well the model generalizes to new, unseen data.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression:\n",
        "Regularization Strength (C)\n",
        "\n",
        "Definition: The C parameter controls the strength of regularization applied to the model. Regularization helps prevent overfitting by penalizing large coefficients in the model.\n",
        "\n",
        "Smaller values of C: Stronger regularization (more penalization), which means the model will try to keep coefficients smaller and prevent overfitting.\n",
        "\n",
        "Larger values of C: Weaker regularization, allowing the model to fit the training data more closely (can lead to overfitting if too large).\n",
        "\n",
        "Range:\n",
        "𝐶\n",
        "∈\n",
        "(\n",
        "0\n",
        ",\n",
        "∞\n",
        ")\n",
        "C∈(0,∞), but smaller values mean stronger regularization.\n",
        "\n",
        "Penalty Type (penalty)\n",
        "\n",
        "Definition: The penalty parameter specifies the type of regularization to use.\n",
        "\n",
        "'l1': Lasso regularization, which encourages sparse solutions (many coefficients equal to zero), leading to feature selection.\n",
        "\n",
        "'l2': Ridge regularization, which discourages large coefficients but does not force them to be exactly zero.\n",
        "\n",
        "None: No regularization, though this is not typically recommended because it can lead to overfitting.\n",
        "\n",
        "Range: {'l1', 'l2', 'none'}\n",
        "\n",
        "Solver (solver)\n",
        "\n",
        "Definition: The solver is the algorithm used to optimize the loss function during training. Different solvers use different methods for finding the optimal coefficients.\n",
        "\n",
        "'liblinear': Good for small datasets, supports both L1 and L2 regularization.\n",
        "\n",
        "'newton-cg', 'lbfgs', 'saga': These solvers can handle large datasets and are more efficient for larger datasets with many features.\n",
        "\n",
        "'saga': This is the solver for large datasets that can handle both L1 and L2 penalties.\n",
        "\n",
        "Range: {'liblinear', 'newton-cg', 'lbfgs', 'saga'}\n",
        "\n",
        "Max Iterations (max_iter)\n",
        "\n",
        "Definition: The maximum number of iterations for the solver to converge to a solution. If the model has not converged within this number of iterations, it will stop training.\n",
        "\n",
        "Range: Integer value (e.g., 100, 1000).\n",
        "\n",
        "Use case: Use larger values if the model is not converging in fewer iterations, especially with complex models or larger datasets.\n",
        "\n",
        "Tolerance (tol)\n",
        "\n",
        "Definition: The tolerance for stopping criteria. If the improvement in the objective function is smaller than this tolerance, the optimization process will stop.\n",
        "\n",
        "Range: Small positive values, e.g.,\n",
        "10\n",
        "−\n",
        "4\n",
        "10\n",
        "−4\n",
        " ,\n",
        "10\n",
        "−\n",
        "3\n",
        "10\n",
        "−3\n",
        " .\n",
        "\n",
        "Use case: Set lower tolerance for more precise convergence.\n",
        "\n",
        "Class Weight (class_weight)\n",
        "\n",
        "Definition: This parameter assigns weights to the classes, and it’s particularly useful for imbalanced datasets. It can be set to 'balanced' to automatically adjust the weights based on the frequencies of the classes or set to a dictionary with specific weights for each class.\n",
        "\n",
        "Range: {'balanced', or a dictionary like {0: weight_0, 1: weight_1}}\n",
        "\n",
        "Use case: Useful for handling class imbalance in classification tasks.\n",
        "\n",
        "Intercept Scaling (intercept_scaling)\n",
        "\n",
        "Definition: Only used when the solver is 'liblinear'. It controls the scaling of the intercept term. It's an internal parameter used to adjust how much influence the intercept term has on the model.\n",
        "\n",
        "Range: Positive float (default is 1).\n",
        "\n",
        "Methods for Hyperparameter Tuning:\n",
        "1. Grid Search\n",
        "Definition: Grid Search exhaustively searches through a manually specified set of hyperparameters. It evaluates the performance of the model for every combination of hyperparameters in the search space.\n",
        "\n",
        "Use case: Good when you have a small hyperparameter space and want to systematically try different combinations.\n",
        "2. Random Search\n",
        "Definition: Instead of evaluating every possible combination, Random Search selects random combinations of hyperparameters. This can be more efficient when the hyperparameter space is large.\n",
        "\n",
        "Use case: Suitable when you have a large hyperparameter space and want to avoid the computational expense of grid search.\n",
        "3. Bayesian Optimization\n",
        "Definition: A probabilistic model is used to find the optimal hyperparameters more efficiently by considering the past evaluations. Bayesian optimization is a more advanced method compared to grid search and random search.\n",
        "\n",
        "Use case: Good for optimizing hyperparameters in high-dimensional spaces.\n",
        "\n",
        "How Hyperparameter Tuning Affects Logistic Regression:\n",
        "Model Performance: Tuning hyperparameters like C and penalty can drastically improve the model's performance, ensuring it balances underfitting and overfitting.\n",
        "\n",
        "Training Time: The solver choice and max iterations affect training time. For instance, solvers like 'newton-cg' are typically faster for larger datasets, whereas 'liblinear' may be slower but is more suitable for smaller datasets.\n",
        "\n",
        "Convergence: Tuning max_iter and tol ensures the model converges properly. If the model stops prematurely, it may result in suboptimal parameters.\n",
        "\n",
        "Conclusion:\n",
        "Hyperparameter tuning in Logistic Regression helps you find the best configuration to improve model performance.\n",
        "\n",
        "Common hyperparameters include regularization strength (C), penalty type, solver, max iterations, and class weights.\n",
        "\n",
        "Methods like Grid Search, Random Search, and Bayesian Optimization are used for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fay1mi0jL6Cj"
      },
      "source": [
        "Q(14)What are different solvers in Logistic Regression? Which one should be used?\n",
        "- In Logistic Regression, the solver is the algorithm used to optimize the loss function and find the best model coefficients. Different solvers employ different optimization techniques, and the choice of solver can impact model training time, convergence, and performance. Here's a breakdown of the different solvers available in Logistic Regression and guidance on which one to use.\n",
        "\n",
        "Solvers in Logistic Regression\n",
        "'liblinear':\n",
        "\n",
        "Description: 'liblinear' is a solver based on coordinate descent. It is suitable for small datasets and is the default solver in sklearn when using L1 regularization (Lasso).\n",
        "\n",
        "Use case:\n",
        "\n",
        "Works well for smaller datasets (e.g., fewer than 100,000 samples).\n",
        "\n",
        "Preferred when using L1 regularization or when the dataset is small and you're focusing on sparse solutions (sparse coefficients).\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Good for smaller datasets and L1 regularization.\n",
        "\n",
        "Can handle both L1 and L2 regularization.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Slower for larger datasets.\n",
        "\n",
        "May not perform as well on large or high-dimensional data.\n",
        "\n",
        "'newton-cg':\n",
        "\n",
        "Description: 'newton-cg' is a second-order optimization method that uses Newton's method. It is faster and more efficient than first-order methods (like liblinear) for large datasets.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Ideal for L2 regularization (Ridge).\n",
        "\n",
        "Suitable for large datasets and high-dimensional feature spaces (e.g., when the number of features is greater than the number of samples).\n",
        "\n",
        "Works well for multi-class classification using the one-vs-rest approach.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Converges faster than first-order solvers like 'liblinear'.\n",
        "\n",
        "Efficient for larger datasets and high-dimensional data.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "May not work well with L1 regularization.\n",
        "\n",
        "Requires more memory.\n",
        "\n",
        "'lbfgs':\n",
        "\n",
        "Description: 'lbfgs' (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) is another second-order method that is faster and more efficient for large datasets. It's an approximation of the BFGS algorithm, designed to use limited memory for large datasets.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Works for L2 regularization (Ridge).\n",
        "\n",
        "Suitable for large datasets and high-dimensional spaces, as it is memory-efficient.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Efficient for large datasets and high-dimensional data.\n",
        "\n",
        "Memory-efficient compared to 'newton-cg'.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "May not handle L1 regularization as well.\n",
        "\n",
        "May require more iterations for convergence on complex datasets.\n",
        "\n",
        "'saga':\n",
        "\n",
        "Description: 'saga' is an optimization algorithm based on stochastic gradient descent (SGD), designed to handle large datasets efficiently. It supports both L1 and L2 regularization and can handle large sparse datasets, such as in text classification or when dealing with many features.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Works well with L1, L2, and Elastic Net regularization.\n",
        "\n",
        "Suitable for large datasets, sparse datasets, and when L1 regularization is needed.\n",
        "\n",
        "Supports multi-class classification using a one-vs-rest scheme.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Efficient for large datasets, including sparse data.\n",
        "\n",
        "Supports L1, L2, and Elastic Net regularization.\n",
        "\n",
        "Often faster for large datasets with many features.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "May not be as fast as 'lbfgs' for dense datasets.\n",
        "\n",
        "It may converge slower than other solvers for smaller datasets.\n",
        "\n",
        "When to Use Which Solver?\n",
        "'liblinear':\n",
        "\n",
        "Use when the dataset is small to medium-sized.\n",
        "\n",
        "Preferable for L1 regularization (Lasso).\n",
        "\n",
        "Good for binary classification problems, especially with fewer features or a small number of training samples.\n",
        "\n",
        "'newton-cg':\n",
        "\n",
        "Use when the dataset is large and you have L2 regularization (Ridge).\n",
        "\n",
        "Works well for multi-class problems and high-dimensional feature spaces.\n",
        "\n",
        "Ideal for dense datasets (i.e., most features are non-zero).\n",
        "\n",
        "'lbfgs':\n",
        "\n",
        "Use when the dataset is large or has many features and you want L2 regularization.\n",
        "\n",
        "It's a good default solver for larger datasets with many features that don't require L1 regularization.\n",
        "\n",
        "Preferred for situations where memory usage is a concern.\n",
        "\n",
        "'saga':\n",
        "\n",
        "Use when the dataset is very large or sparse (e.g., text classification).\n",
        "\n",
        "Supports L1, L2, and Elastic Net regularization.\n",
        "\n",
        "Ideal for problems with large feature spaces or when L1 regularization is desired.\n",
        "\n",
        "Summary Table\n",
        "\n",
        "Solver\tBest For\tRegularization Support\tDataset Size\tMemory Usage\tMulti-class Support\n",
        "'liblinear'\tSmall datasets, L1 regularization (Lasso)\tL1, L2\tSmall\tLow\tYes\n",
        "'newton-cg'\tLarge datasets, L2 regularization (Ridge)\tL2\tLarge\tHigh\tYes\n",
        "'lbfgs'\tLarge, high-dimensional datasets, L2 regularization\tL2\tLarge\tModerate\tYes\n",
        "'saga'\tLarge/sparse datasets, L1 regularization, Elastic Net\tL1, L2, Elastic Net\tVery Large\tModerate\tYes\n",
        "Conclusion:\n",
        "'liblinear' is a good choice for smaller datasets and L1 regularization.\n",
        "\n",
        "'newton-cg' and 'lbfgs' are better suited for larger datasets with L2 regularization.\n",
        "\n",
        "'saga' is the best choice for very large or sparse datasets, especially when using L1 regularization or Elastic Net regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ_II8mAMMoH"
      },
      "source": [
        "Q(15)How is Logistic Regression extended for multiclass classification?\n",
        "- In Logistic Regression, the model is inherently designed for binary classification (i.e., two classes). However, it can be extended to handle multiclass classification (i.e., more than two classes) using two primary strategies:\n",
        "\n",
        "1. One-vs-Rest (OvR) or One-vs-All (OvA)\n",
        "Description: In this approach, a separate binary classifier is trained for each class in the dataset. For a classification problem with\n",
        "𝑘\n",
        "k classes, you train\n",
        "𝑘\n",
        "k separate logistic regression classifiers.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Each classifier is trained to distinguish one class from the rest of the classes. For example, if there are three classes A, B, and C, you will train three classifiers:\n",
        "\n",
        "Classifier 1: A vs (B + C)\n",
        "\n",
        "Classifier 2: B vs (A + C)\n",
        "\n",
        "Classifier 3: C vs (A + B)\n",
        "\n",
        "During prediction, the classifier that outputs the highest probability determines the predicted class.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple to implement.\n",
        "\n",
        "Easily interpretable since each classifier works in a binary setting.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Can be computationally expensive if the number of classes is large.\n",
        "\n",
        "Each classifier is independent, so it does not account for correlations between classes (i.e., it doesn't model the relationship between classes directly).\n",
        "\n",
        "Use case: This method is commonly used for multiclass classification problems, especially when there is a need to keep each class classifier interpretable.\n",
        "\n",
        "2. One-vs-One (OvO)\n",
        "Description: In this approach, a binary classifier is trained for every pair of classes in the dataset. For\n",
        "𝑘\n",
        "k classes, you will train\n",
        "𝑘\n",
        "(\n",
        "𝑘\n",
        "−\n",
        "1\n",
        ")\n",
        "2\n",
        "2\n",
        "k(k−1)\n",
        "​\n",
        "  classifiers (one for each pair of classes).\n",
        "\n",
        "How it works:\n",
        "\n",
        "For example, if there are three classes A, B, and C, you train the following classifiers:\n",
        "\n",
        "Classifier 1: A vs B\n",
        "\n",
        "Classifier 2: A vs C\n",
        "\n",
        "Classifier 3: B vs C\n",
        "\n",
        "During prediction, each classifier votes for a class, and the class with the majority of votes is selected as the predicted class.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Can potentially provide better performance because each classifier is focused on distinguishing between just two classes at a time.\n",
        "\n",
        "It might be better at capturing complex class relationships than OvR because it trains on specific pairs of classes.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Computationally expensive, especially when there are a large number of classes. For example, with 10 classes, you'll need 45 classifiers, which can be costly.\n",
        "\n",
        "More complex to implement and interpret than OvR.\n",
        "\n",
        "Use case: This method is used in situations where distinguishing between every possible pair of classes is important, though it is typically less popular than OvR due to the computational cost.\n",
        "\n",
        "3. Multinomial Logistic Regression (Softmax Regression)\n",
        "Description: In Multinomial Logistic Regression, the logistic regression model is directly extended to multiple classes. Instead of predicting the probability of a class as in binary logistic regression, we predict the probability of each class using the softmax function, which normalizes the outputs to sum to 1. This allows the model to predict the class probabilities for more than two classes simultaneously.\n",
        "\n",
        "How it works:\n",
        "\n",
        "For a classification problem with\n",
        "𝑘\n",
        "k classes, instead of having a single decision function (like the log-odds in binary logistic regression), you have\n",
        "𝑘\n",
        "k decision functions (one for each class). Each function gives the log-odds of each class relative to a baseline class.\n",
        "\n",
        "The softmax function is used to convert the log-odds into probabilities:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑖\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑗\n",
        ")\n",
        "P(y=i∣x)=\n",
        "∑\n",
        "j=1\n",
        "k\n",
        "​\n",
        " exp(x\n",
        "T\n",
        " θ\n",
        "j\n",
        "​\n",
        " )\n",
        "exp(x\n",
        "T\n",
        " θ\n",
        "i\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "Here,\n",
        "𝑥\n",
        "x is the feature vector, and\n",
        "𝜃\n",
        "𝑖\n",
        "θ\n",
        "i\n",
        "​\n",
        "  is the coefficient vector for class\n",
        "𝑖\n",
        "i. The denominator sums the exponentials of the scores for all classes, ensuring that the output probabilities sum to 1.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Directly models multiclass classification in one step, without needing multiple classifiers.\n",
        "\n",
        "More efficient and scalable for problems with many classes.\n",
        "\n",
        "Probabilities are well-calibrated and sum to 1.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Works best when classes are mutually exclusive, i.e., each sample belongs to one and only one class.\n",
        "\n",
        "The model complexity increases with the number of classes.\n",
        "\n",
        "Use case: This is the most commonly used method for multiclass classification in Logistic Regression (and is implemented in libraries like scikit-learn using the multinomial option for the solver).\n",
        "\n",
        "Comparison of the Methods\n",
        "\n",
        "Method\tNumber of Models\tTraining Complexity\tPrediction Complexity\tBest Used For\n",
        "One-vs-Rest (OvR)\n",
        "𝑘\n",
        "k models\tModerate\tFast\tSimple problems with many classes, interpretable models\n",
        "One-vs-One (OvO)\n",
        "𝑘\n",
        "(\n",
        "𝑘\n",
        "−\n",
        "1\n",
        ")\n",
        "2\n",
        "2\n",
        "k(k−1)\n",
        "​\n",
        "  models\tHigh due to multiple classifiers\tModerate (voting)\tProblems with complex class boundaries, small number of classes\n",
        "Multinomial Logistic Regression\t1 model\tModerate to high, depending on number of classes\tModerate, direct probabilities\tLarge number of classes, when classes are mutually exclusive.\n",
        " Conclusion:\n",
        "- One-vs-Rest (OvR) is often a simple and efficient choice, especially when classes are linearly separable.\n",
        "\n",
        "- One-vs-One (OvO) can sometimes provide better performance but is computationally expensive, especially with many classes.\n",
        "\n",
        "- Multinomial Logistic Regression (Softmax) is the most efficient and scalable method, especially for problems with many classes, and it directly models the multiclass probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jox7xVVmNRqF"
      },
      "source": [
        "Q(16)What are the advantages and disadvantages of Logistic Regression?\n",
        "- Logistic Regression is one of the most commonly used algorithms for classification tasks, particularly binary classification. Like all models, it has its advantages and disadvantages. Here’s a detailed breakdown:\n",
        "\n",
        "Advantages of Logistic Regression:\n",
        "Simple and Easy to Understand:\n",
        "\n",
        "Logistic Regression is easy to implement and interpret. The model provides clear insights into how the independent variables (features) affect the dependent variable (target).\n",
        "\n",
        "The coefficients of the model can be interpreted as the change in the log-odds of the target variable for a one-unit change in the predictor.\n",
        "\n",
        "Fast to Train:\n",
        "\n",
        "Logistic Regression is computationally efficient and can be trained quickly, especially for datasets with a relatively small number of features.\n",
        "\n",
        "This makes it an excellent choice for real-time applications and when quick results are required.\n",
        "\n",
        "Works Well with Linearly Separable Data:\n",
        "\n",
        "Logistic Regression works well when the classes are linearly separable. If the decision boundary between classes is a straight line (or a hyperplane in higher dimensions), Logistic Regression can achieve high accuracy.\n",
        "\n",
        "Probabilistic Interpretation:\n",
        "\n",
        "Logistic Regression outputs probabilities, which gives more information than simply assigning a class label. This can be useful in scenarios where you need to know how confident the model is in its predictions.\n",
        "\n",
        "The predicted probabilities can be used for threshold tuning to adjust for precision/recall balance.\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Logistic Regression can be regularized with L1 (Lasso) or L2 (Ridge) penalties to prevent overfitting. Regularization helps when dealing with datasets that have a large number of features or high multicollinearity.\n",
        "\n",
        "Works Well with Large Datasets:\n",
        "\n",
        "Logistic Regression tends to perform well with large datasets, especially when the number of features is not excessively large.\n",
        "\n",
        "It can be scaled to handle large volumes of data by using stochastic gradient descent (SGD) or other optimization methods.\n",
        "\n",
        "Less Prone to Overfitting (with regularization):\n",
        "\n",
        "By using techniques like L2 regularization or L1 regularization, logistic regression can avoid overfitting, particularly when the model is dealing with many features or complex datasets.\n",
        "\n",
        "Easy to Extend for Multiclass:\n",
        "\n",
        "Logistic Regression can be easily extended to multiclass classification through methods like One-vs-Rest (OvR) or Multinomial Logistic Regression, allowing it to handle problems with more than two classes.\n",
        "\n",
        "Disadvantages of Logistic Regression:\n",
        "Assumes Linearity:\n",
        "\n",
        "Logistic Regression assumes that there is a linear relationship between the input features and the log-odds of the outcome. This can limit its performance if the data contains nonlinear relationships.\n",
        "\n",
        "If the data is highly non-linear, Logistic Regression might underperform compared to more flexible models like Decision Trees or Random Forests.\n",
        "\n",
        "Sensitive to Outliers:\n",
        "\n",
        "Logistic Regression can be sensitive to outliers. Outliers in the data may distort the estimated coefficients, leading to biased or inaccurate predictions.\n",
        "\n",
        "This is particularly problematic in situations where the data has extreme values or noise.\n",
        "\n",
        "Needs Feature Engineering:\n",
        "\n",
        "Since Logistic Regression assumes a linear relationship, it often requires careful feature engineering to work well with complex data. For instance, polynomial features or interaction terms might need to be created manually to capture more complex patterns.\n",
        "\n",
        "It may also need normalization/scaling of features to avoid numerical instability or differences in magnitude between features.\n",
        "\n",
        "Doesn't Handle Complex Relationships Well:\n",
        "\n",
        "Logistic Regression is not effective when the relationship between the features and the target variable is very complex or when features interact in complicated ways. In these cases, more flexible models like Decision Trees, Random Forests, or Neural Networks might perform better.\n",
        "\n",
        "Limited to Binary Outcomes (in basic form):\n",
        "\n",
        "Basic Logistic Regression is inherently designed for binary classification (two classes). While it can be extended to multiclass problems, these extensions are often based on simpler, binary classifiers (like One-vs-Rest), which may not capture more complex relationships in the data as well as other methods.\n",
        "\n",
        "Multicollinearity Problems:\n",
        "\n",
        "Logistic Regression can suffer from multicollinearity (when independent variables are highly correlated with each other), which makes the model coefficients unstable and difficult to interpret.\n",
        "\n",
        "This can lead to problems in model estimation and could require techniques like Principal Component Analysis (PCA) or Ridge regularization to deal with collinearity.\n",
        "\n",
        "Poor Performance with High-Dimensional Data (without regularization):\n",
        "\n",
        "Logistic Regression can struggle with high-dimensional data (i.e., when the number of features is much larger than the number of observations), especially without using regularization to prevent overfitting.\n",
        "\n",
        "Assumes Independence of Features:\n",
        "\n",
        "Logistic Regression assumes that the features are independent of one another, but in practice, features often have interactions or dependencies. If the feature independence assumption is violated, the model’s performance may degrade.\n",
        "\n",
        "Not Robust to Noise:\n",
        "\n",
        "Logistic Regression can be less robust in the presence of noisy data or when the classes are imbalanced. In such cases, models like Random Forests or Gradient Boosting may perform better as they are more robust to noise and can handle imbalances more effectively.\n",
        "\n",
        "When to Use Logistic Regression:\n",
        "Simple binary classification tasks where a linear relationship exists between the input features and the output.\n",
        "\n",
        "Multiclass classification when the relationship between features and the target is approximately linear.\n",
        "\n",
        "Problems where you need probabilistic interpretation of predictions.\n",
        "\n",
        "When interpretability is important, as the coefficients of the model can be easily understood and interpreted.\n",
        "\n",
        "When you have a small to medium-sized dataset and a low-to-moderate number of features.\n",
        "\n",
        "When Not to Use Logistic Regression:\n",
        "When the dataset contains complex nonlinear relationships that cannot be captured by a linear model.\n",
        "\n",
        "When the data is noisy or contains outliers that could affect model performance.\n",
        "\n",
        "For high-dimensional data (many features), unless proper regularization techniques are applied.\n",
        "\n",
        "When you need a model that can handle feature interactions and nonlinear relationships naturally, like Decision Trees, Random Forests, or Support Vector Machines (SVM).\n",
        "\n",
        "Conclusion:\n",
        "Logistic Regression is a powerful and efficient tool for classification tasks, especially when the relationship between features and the target is linear. It is particularly useful for binary classification problems and offers clear probabilistic predictions. However, it has limitations in dealing with nonlinear relationships, high-dimensional data, and multicollinearity. Understanding these pros and cons helps in choosing the right model based on the nature of the data and the problem you're trying to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ1JIUoGNiwY"
      },
      "source": [
        "Q(17)What are some use cases of Logistic Regression?\n",
        "- Logistic Regression is a versatile algorithm and is widely used in many different domains. Below are some key use cases of Logistic Regression in various industries:\n",
        "\n",
        "1. Medical Diagnosis:\n",
        "Problem: Predicting whether a patient has a certain disease (e.g., cancer, heart disease, diabetes) based on diagnostic test results and medical history.\n",
        "\n",
        "Use case: Logistic Regression is commonly used in healthcare to model binary outcomes such as:\n",
        "\n",
        "Heart disease prediction: Based on factors like age, cholesterol levels, blood pressure, and other health markers.\n",
        "\n",
        "Cancer prediction: Classifying whether a tumor is malignant or benign using data from medical imaging or biopsy results.\n",
        "\n",
        "Diabetes prediction: Using features such as age, BMI, family history, etc., to predict whether an individual is at risk of developing diabetes.\n",
        "\n",
        "2. Credit Scoring and Risk Assessment:\n",
        "Problem: Predicting whether a person will default on a loan or credit card payment.\n",
        "\n",
        "Use case: Banks and financial institutions use Logistic Regression to assess creditworthiness:\n",
        "\n",
        "Loan default prediction: Using historical data on customers' financial history (e.g., income, credit history, loan amount) to predict whether a borrower is likely to default on a loan.\n",
        "\n",
        "Fraud detection: Identifying fraudulent transactions by classifying them as either legitimate or fraudulent based on transaction data and customer behavior.\n",
        "\n",
        "3. Marketing and Customer Segmentation:\n",
        "Problem: Predicting whether a customer will purchase a product or subscribe to a service based on historical behavior.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Churn prediction: In subscription-based businesses, Logistic Regression can predict whether a customer is likely to cancel their subscription (churn).\n",
        "\n",
        "Customer purchase behavior: Predicting whether a customer is likely to buy a product (e.g., based on website interactions, demographic data, and previous purchase history).\n",
        "\n",
        "Targeted marketing: Classifying customers who are likely to respond to specific marketing campaigns, helping businesses optimize advertising and promotions.\n",
        "\n",
        "4. Spam Email Classification:\n",
        "Problem: Classifying emails as \"spam\" or \"non-spam\" based on the content and metadata.\n",
        "\n",
        "Use case: Logistic Regression is used to determine whether an email is spam or not by analyzing various features such as:\n",
        "\n",
        "Email subject line\n",
        "\n",
        "Keywords and phrases in the body\n",
        "\n",
        "Sender’s address\n",
        "\n",
        "Email metadata like time of sending, frequency, etc.\n",
        "\n",
        "5. Sentiment Analysis:\n",
        "Problem: Predicting the sentiment (positive, negative, or neutral) of a given piece of text, such as a product review or a tweet.\n",
        "\n",
        "Use case: Logistic Regression can be used to classify the sentiment of textual data by considering features like:\n",
        "\n",
        "Presence of certain keywords or phrases.\n",
        "\n",
        "Frequency of positive/negative words.\n",
        "\n",
        "Text length, punctuation, or other linguistic features.\n",
        "\n",
        "Sentiment analysis is widely used in marketing and brand management to assess customer feedback.\n",
        "\n",
        "6. Social Media Classification:\n",
        "Problem: Classifying social media content into categories (e.g., hate speech, bullying, offensive content).\n",
        "\n",
        "Use case: Logistic Regression can be applied to detect harmful or unwanted content on platforms like Facebook, Twitter, or YouTube:\n",
        "\n",
        "Content moderation: Identifying and filtering offensive language or hate speech.\n",
        "\n",
        "Fake news detection: Classifying articles as true or false based on their content and metadata.\n",
        "\n",
        "7. Election Forecasting:\n",
        "Problem: Predicting election outcomes or voting patterns based on demographic and historical data.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Logistic Regression can be used in political campaigns or polling companies to predict voter turnout or party preference based on variables such as age, gender, region, socioeconomic status, etc.\n",
        "\n",
        "It can also be used to predict the probability of a voter voting for a particular party or candidate based on survey data.\n",
        "\n",
        "8. Human Resources:\n",
        "Problem: Predicting employee behavior, such as whether an employee will leave the company or not.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Employee attrition prediction: Using factors like tenure, salary, department, performance reviews, and other features to predict whether an employee is likely to leave the company.\n",
        "\n",
        "Recruitment: Logistic Regression can help predict whether a job candidate is likely to succeed in a role based on their resume or historical hiring data.\n",
        "\n",
        "9. Quality Control and Manufacturing:\n",
        "Problem: Predicting whether a product meets quality standards or will fail during production.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Defect detection: Logistic Regression can be used to predict whether a product will pass quality control checks based on features such as raw material quality, production conditions, and past defect rates.\n",
        "\n",
        "Predictive maintenance: In manufacturing, it can predict whether a machine is likely to break down, helping with maintenance scheduling and reducing downtime.\n",
        "\n",
        "10. Disease Outbreak Prediction:\n",
        "Problem: Predicting the likelihood of a disease outbreak in a specific region based on environmental and historical data.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Logistic Regression can be used in epidemiology to predict whether an outbreak (e.g., flu, COVID-19, etc.) will occur in a particular region by using factors like weather conditions, population density, travel patterns, and past outbreak data.\n",
        "\n",
        "11. Image Classification (in some cases):\n",
        "Problem: Classifying images into categories (e.g., classifying images of cats vs. dogs).\n",
        "\n",
        "Use case: While deep learning models (like CNNs) are more popular for image classification, Logistic Regression can still be used for simpler image classification tasks, especially when the features are extracted manually from the image data (e.g., using edge detection, pixel intensities, etc.).\n",
        "\n",
        "12. Real-Time Prediction:\n",
        "Problem: Predicting outcomes in real-time applications, such as recommending products or providing real-time user feedback.\n",
        "\n",
        "Use case:\n",
        "\n",
        "Real-time fraud detection: Logistic Regression can predict fraudulent transactions as they occur in real-time by analyzing transaction features (e.g., amount, location, time, and user history).\n",
        "\n",
        "Product recommendations: It can be used to classify whether a customer will likely buy a product based on their browsing history.\n",
        "\n",
        "Summary of Use Cases:\n",
        "Logistic Regression is widely used in any domain that involves binary or multiclass classification. Its use cases span across healthcare, finance, marketing, social media, and many other industries. It is particularly effective in cases where the relationship between the features and the target is linear or approximately linear, and when the goal is to interpret the impact of each feature on the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DROpNZjUNxK1"
      },
      "source": [
        "Q(18)What is the difference between Softmax Regression and Logistic Regression?\n",
        "- The main difference between Softmax Regression and Logistic Regression lies in the type of classification they are designed for and how they model the relationship between features and classes.\n",
        "\n",
        "Here’s a detailed breakdown of their differences:\n",
        "\n",
        "1. Type of Classification\n",
        "Logistic Regression:\n",
        "\n",
        "Binary Classification: Logistic Regression is typically used for binary classification tasks, where the goal is to predict one of two possible outcomes (e.g., yes/no, 0/1).\n",
        "\n",
        "Output: The model predicts the probability of a sample belonging to a single class (usually class 1) based on the logistic function (sigmoid function). The probability for class 0 is simply the complement (1 - probability for class 1).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(x\n",
        "T\n",
        " θ)\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "The logistic function maps the linear combination of features to a value between 0 and 1, interpreted as a probability.\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression):\n",
        "\n",
        "Multiclass Classification: Softmax Regression, also known as Multinomial Logistic Regression, is used for multiclass classification tasks, where there are more than two possible classes (e.g., classifying images of animals as cat, dog, or rabbit).\n",
        "\n",
        "Output: The model predicts a probability for each of the classes. The sum of the probabilities for all classes will be 1.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑖\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑗\n",
        ")\n",
        "P(y=i∣x)=\n",
        "∑\n",
        "j=1\n",
        "k\n",
        "​\n",
        " exp(x\n",
        "T\n",
        " θ\n",
        "j\n",
        "​\n",
        " )\n",
        "exp(x\n",
        "T\n",
        " θ\n",
        "i\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "Here,\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=i∣x) represents the probability that the sample belongs to class\n",
        "𝑖\n",
        "i,\n",
        "𝜃\n",
        "𝑖\n",
        "θ\n",
        "i\n",
        "​\n",
        "  represents the coefficients for class\n",
        "𝑖\n",
        "i, and\n",
        "𝑘\n",
        "k is the total number of classes. The softmax function normalizes the output of all classes into a probability distribution.\n",
        "\n",
        "2. Mathematical Model\n",
        "Logistic Regression:\n",
        "\n",
        "In logistic regression, we model the log-odds of the binary target using a single linear equation:\n",
        "\n",
        "log-odds\n",
        "=\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "log-odds=x\n",
        "T\n",
        " θ\n",
        "The output of the logistic function is interpreted as the probability of belonging to one class, while the probability of the other class is simply\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "1−P.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "In softmax regression, we model the log-odds for each class relative to a baseline class. For each class\n",
        "𝑖\n",
        "i, the model computes the log-odds using a separate linear equation for each class:\n",
        "\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑖\n",
        "x\n",
        "T\n",
        " θ\n",
        "i\n",
        "​\n",
        "\n",
        "The softmax function then converts these log-odds into probabilities, ensuring that the sum of the probabilities for all classes is 1.\n",
        "\n",
        "3. Use Case\n",
        "Logistic Regression:\n",
        "\n",
        "Binary classification tasks (e.g., predicting whether an email is spam or not).\n",
        "\n",
        "It can also be extended to multiclass classification using strategies like One-vs-Rest or One-vs-One, but this doesn't involve directly using the logistic regression model for multiclass classification.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass classification problems, where there are more than two possible classes (e.g., classifying an image as a cat, dog, or rabbit).\n",
        "\n",
        "It directly handles multiclass problems by predicting the probabilities of all classes simultaneously.\n",
        "\n",
        "4. Prediction Output\n",
        "Logistic Regression:\n",
        "\n",
        "Binary output: Logistic Regression produces a probability for the positive class (class 1) and a complement probability for the negative class (class 0). These two probabilities must sum to 1.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(x\n",
        "T\n",
        " θ)\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass output: Softmax Regression produces a probability distribution over multiple classes (e.g., class 1, class 2, ..., class\n",
        "𝑘\n",
        "k).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑖\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑥\n",
        "𝑇\n",
        "𝜃\n",
        "𝑗\n",
        ")\n",
        "P(y=i∣x)=\n",
        "∑\n",
        "j=1\n",
        "k\n",
        "​\n",
        " exp(x\n",
        "T\n",
        " θ\n",
        "j\n",
        "​\n",
        " )\n",
        "exp(x\n",
        "T\n",
        " θ\n",
        "i\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "5. Training Process\n",
        "Logistic Regression:\n",
        "\n",
        "Binary: The training process involves finding the optimal coefficients (\n",
        "𝜃\n",
        "θ) that minimize the binary log-likelihood loss function (or equivalently, maximize the likelihood of the data).\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "L(θ)=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " log(\n",
        "p\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−\n",
        "p\n",
        "^\n",
        "​\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass: The training process involves finding the optimal coefficients for each class that minimize the cross-entropy loss (a generalization of the binary log-likelihood loss to multiple classes).\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑐\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑦\n",
        "𝑖\n",
        "𝑐\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "^\n",
        "𝑖\n",
        "𝑐\n",
        ")\n",
        "L(θ)=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        "  \n",
        "c=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " y\n",
        "ic\n",
        "​\n",
        " log(\n",
        "p\n",
        "^\n",
        "​\n",
        "  \n",
        "ic\n",
        "​\n",
        " )\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "𝑐\n",
        "y\n",
        "ic\n",
        "​\n",
        "  is the true label (0 or 1) for class\n",
        "𝑐\n",
        "c and\n",
        "𝑝\n",
        "^\n",
        "𝑖\n",
        "𝑐\n",
        "p\n",
        "^\n",
        "​\n",
        "  \n",
        "ic\n",
        "​\n",
        "  is the predicted probability of class\n",
        "𝑐\n",
        "c for the\n",
        "𝑖\n",
        "i-th sample.\n",
        "\n",
        "6. Extending to Multiclass\n",
        "Logistic Regression:\n",
        "\n",
        "Logistic Regression can be extended to handle multiple classes using techniques like:\n",
        "\n",
        "One-vs-Rest (OvR): One binary classifier per class.\n",
        "\n",
        "One-vs-One (OvO): One binary classifier per pair of classes.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Softmax Regression natively handles multiclass problems by providing a direct probability distribution over all classes, making it the preferred approach for multiclass classification.\n",
        "\n",
        "- Summary Table:\n",
        "\n",
        "- Feature\tLogistic Regression\tSoftmax Regression (Multinomial Logistic Regression)\n",
        "- Type of Classification\tBinary classification\tMulticlass classification\n",
        "Output\tProbability of class 1 (binary)\tProbability distribution across all classes\n",
        "Mathematical Model\tLogistic function (sigmoid)\tSoftmax function (generalization of sigmoid)\n",
        "Use Case\tPredicting binary outcomes (yes/no, 0/1)\tPredicting outcomes from multiple classes (e.g., cat, dog, rabbit)\n",
        "Prediction Output\tOne probability (class 1)\tMultiple probabilities (one per class)\n",
        "Training\tBinary log-likelihood loss\tCross-entropy loss\n",
        "Multiclass Handling\tExtended via One-vs-Rest or One-vs-One\tNatively handles multiple classes via softmax\n",
        "When to Use Which:\n",
        "Use Logistic Regression when you have a binary classification problem, and you need a simple, interpretable model.\n",
        "\n",
        "Use Softmax Regression (Multinomial Logistic Regression) when you have a multiclass classification problem and need a model that can directly predict the probabilities of multiple classes without relying on binary classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iupa8xtSODXS"
      },
      "source": [
        "Q(19)How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "When deciding between One-vs-Rest (OvR) (also called One-vs-All (OvA)) and Softmax Regression for multiclass classification, there are several factors to consider. Both are strategies for handling multiclass classification, but they have different approaches and are suited for different scenarios. Here's a breakdown of each method, along with factors that can help you choose between them:\n",
        "\n",
        "1. Approach\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Concept: In One-vs-Rest, a separate binary classifier is trained for each class. Each classifier predicts whether an instance belongs to that particular class or not, with the rest of the classes treated as the \"negative\" class.\n",
        "\n",
        "Prediction: After training, for a given test sample, the classifier that produces the highest probability is chosen as the predicted class.\n",
        "\n",
        "Example: If you have 3 classes (Class A, Class B, Class C), you will train 3 separate binary classifiers:\n",
        "\n",
        "Classifier 1: Class A vs (Class B and Class C)\n",
        "\n",
        "Classifier 2: Class B vs (Class A and Class C)\n",
        "\n",
        "Classifier 3: Class C vs (Class A and Class B)\n",
        "\n",
        "Final Output: The class with the highest predicted probability across all classifiers is chosen.\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression):\n",
        "\n",
        "Concept: Softmax Regression is a direct approach for multiclass classification where a single model is trained to predict a probability distribution over all classes simultaneously.\n",
        "\n",
        "Prediction: The model outputs probabilities for each class, and the class with the highest probability is chosen as the predicted class. It uses the softmax function to ensure that the predicted probabilities sum to 1.\n",
        "\n",
        "Example: If you have 3 classes (Class A, Class B, Class C), the model outputs the probability distribution over all 3 classes and picks the class with the highest probability.\n",
        "\n",
        "2. Computational Efficiency\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Pros:\n",
        "\n",
        "Independent Classifiers: Each classifier is independent, so you can train them separately, and they are typically faster to train for smaller datasets.\n",
        "\n",
        "Scalability: If you're working with many classes, OvR can be easier to scale since you can parallelize training the classifiers.\n",
        "\n",
        "Cons:\n",
        "\n",
        "More Model Training: It requires training multiple classifiers (one per class), which can increase training time, especially with large numbers of classes.\n",
        "\n",
        "Resource Intensive: For large-scale problems, training multiple binary classifiers can consume significant computational resources.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Single Model: Only one model is trained for all classes, so it's more efficient in terms of both memory and training time when dealing with a large number of classes.\n",
        "\n",
        "Unified Framework: It optimizes a single cost function across all classes, making it computationally more efficient for tasks with many classes.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Complexity: The training process for Softmax can be computationally more intensive per iteration than a single binary classifier in OvR, especially for very large datasets.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Challenges: OvR classifiers treat the classes as binary problems, which can exacerbate issues with class imbalance. If one class is much larger than others, the classifier for that class may dominate the decision-making process.\n",
        "\n",
        "Mitigation: Techniques like class weighting or oversampling can help address class imbalance in OvR, but it still remains a challenge for each binary classifier.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Advantages: Softmax Regression handles class imbalance in a more unified way because it computes a probability distribution over all classes simultaneously. It uses the cross-entropy loss function, which can incorporate class weights, making it better suited for imbalanced class distributions.\n",
        "\n",
        "4. Model Performance\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Pros:\n",
        "\n",
        "It can work well in practice, especially when using powerful classifiers (like SVMs or Random Forests) for each binary classifier.\n",
        "\n",
        "It can be more interpretable because each binary classifier focuses on one class at a time.\n",
        "\n",
        "Cons:\n",
        "\n",
        "It may lead to suboptimal performance because each classifier is trained in isolation, without considering the interdependencies between the classes.\n",
        "\n",
        "The decision boundaries between classes may not be as well defined as in a Softmax model, where the classes are modeled together.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Softmax Regression generally provides better performance when the classes have dependencies or overlap, since all classes are modeled jointly in a single model.\n",
        "\n",
        "It captures the relationships between classes more effectively because the model learns the entire probability distribution.\n",
        "\n",
        "Cons:\n",
        "\n",
        "In cases where classes are very distinct or separable, OvR may perform similarly or even better, since OvR classifiers focus only on distinguishing one class from others.\n",
        "\n",
        "5. Flexibility and Interpretation\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Pros:\n",
        "\n",
        "OvR provides clearer interpretations for each class because each binary classifier is focused on a single class. You can easily examine the behavior of each classifier and its decision boundaries.\n",
        "\n",
        "Cons:\n",
        "\n",
        "The lack of interaction between classes in the OvR approach means that it may fail to capture complex patterns where classes are interdependent.\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Pros:\n",
        "\n",
        "Softmax provides a more unified model, which is particularly useful when classes are closely related or share similarities.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Softmax Regression may be harder to interpret, especially if the number of classes is large, as it directly learns the relationships between all classes.\n",
        "\n",
        "6. When to Use Each Method\n",
        "Choose One-vs-Rest (OvR) when:\n",
        "\n",
        "You have a large number of classes and the dataset is relatively small, so training a separate binary classifier for each class might be faster and more manageable.\n",
        "\n",
        "You need more interpretability for each individual class.\n",
        "\n",
        "The classes are relatively independent (i.e., the model can work by treating each class as an isolated binary classification problem).\n",
        "\n",
        "You are using a nonlinear classifier (like SVM or decision trees), which can sometimes work better when each class is modeled independently.\n",
        "\n",
        "Choose Softmax Regression when:\n",
        "\n",
        "You have interdependent or overlapping classes, and you want a unified model that captures the relationships between them.\n",
        "\n",
        "You want a more efficient training process for a large number of classes, especially when using simple linear classifiers.\n",
        "\n",
        "You are dealing with class imbalance, as Softmax naturally handles the distribution of class probabilities and can incorporate class weights to improve performance.\n",
        "\n",
        "Summary Table:\n",
        "\n",
        "Aspect\tOne-vs-Rest (OvR)\tSoftmax Regression\n",
        "Number of Models\tOne binary classifier per class\tSingle model for all classes\n",
        "Computational Cost\tHigher due to multiple classifiers\tMore efficient for large-scale multiclass tasks\n",
        "Handling Class Imbalance\tMay struggle with class imbalance\tBetter handling of class imbalance with class weighting\n",
        "Performance\tCan perform well with independent classes\tTypically better for capturing class interdependencies\n",
        "Interpretability\tEasier to interpret each class’s decision\tLess interpretable, but captures class relationships\n",
        "Scalability\tMore scalable for smaller problems\tMore efficient for large-scale problems\n",
        "Conclusion:\n",
        "Use One-vs-Rest (OvR) if you need a simpler, more interpretable solution for a smaller dataset with independent classes or when using nonlinear classifiers.\n",
        "\n",
        "Use Softmax Regression for large-scale problems, interdependent classes, and when you want a unified model that directly predicts the probability distribution across multiple classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgBLrtPXOVwo"
      },
      "source": [
        "Q(20)How do we interpret coefficients in Logistic Regression?\n",
        "- Interpreting the coefficients in Logistic Regression is crucial for understanding how each predictor (feature) affects the probability of the outcome. Since Logistic Regression is a probabilistic model, the coefficients are related to the log-odds of the event happening, which in turn are transformed into probabilities using the sigmoid function.\n",
        "\n",
        "Let’s break down how to interpret these coefficients:\n",
        "\n",
        "1. The Logistic Regression Model\n",
        "The logistic regression model predicts the log-odds of the event happening (e.g., the probability of a positive class), and the equation is:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "logit(P)=log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "P is the probability of the event occurring (e.g., probability of class 1).\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept (bias term).\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients of the features.\n",
        "\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        "  are the input features.\n",
        "\n",
        "The output of the model is the probability\n",
        "𝑃\n",
        "P, which is given by applying the sigmoid function to the log-odds:\n",
        "\n",
        "𝑃\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "2. Interpreting the Coefficients\n",
        "a. Sign of the Coefficients\n",
        "Positive Coefficients:\n",
        "\n",
        "If the coefficient\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " >0, the feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is positively correlated with the outcome. This means that as the value of\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases, the log-odds of the outcome occurring (class 1) also increase, which translates to a higher probability of the event happening.\n",
        "\n",
        "Negative Coefficients:\n",
        "\n",
        "If the coefficient\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " <0, the feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is negatively correlated with the outcome. As\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases, the log-odds of the outcome decrease, leading to a lower probability of the event happening.\n",
        "\n",
        "b. Magnitude of the Coefficients\n",
        "The magnitude of the coefficient indicates the strength of the relationship between the feature and the log-odds of the outcome.\n",
        "\n",
        "Larger absolute values of coefficients (whether positive or negative) indicate that the feature has a stronger effect on the log-odds and thus the probability.\n",
        "\n",
        "Smaller absolute values indicate that the feature has a weaker effect on the outcome.\n",
        "\n",
        "c. Interpreting the Coefficient in Terms of Odds Ratio\n",
        "The coefficients in logistic regression can be interpreted in terms of odds ratio. The odds ratio is the factor by which the odds of the outcome change for a one-unit increase in the predictor variable, holding all other variables constant.\n",
        "\n",
        "The odds ratio for a feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is given by:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "Interpretation of Odds Ratio:\n",
        "\n",
        "If\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " >0, the odds ratio\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " >1, which means that as\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases, the odds of the event occurring (class 1) increase. Specifically, for each unit increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , the odds of the outcome increase by a factor of\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " .\n",
        "\n",
        "If\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " <0, the odds ratio\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " <1, which means that as\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases, the odds of the event occurring decrease. For each unit increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , the odds of the outcome decrease by a factor of\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " .\n",
        "\n",
        "If\n",
        "𝛽\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " =0, the odds ratio is\n",
        "1\n",
        "1, indicating that the feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  has no effect on the odds of the event occurring.\n",
        "\n",
        "d. Example\n",
        "Let’s consider a logistic regression model with two features: Age and Income. Suppose the fitted coefficients are:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "−\n",
        "4\n",
        "+\n",
        "0.05\n",
        "⋅\n",
        "Age\n",
        "+\n",
        "0.02\n",
        "⋅\n",
        "Income\n",
        "logit(P)=−4+0.05⋅Age+0.02⋅Income\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " ): The intercept is -4, meaning when both Age and Income are 0, the log-odds of the event happening are -4.\n",
        "\n",
        "Coefficient for Age (\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "0.05\n",
        "β\n",
        "1\n",
        "​\n",
        " =0.05): For each additional year of Age, the log-odds of the event happening increase by 0.05.\n",
        "\n",
        "Odds Ratio for Age:\n",
        "𝑒\n",
        "0.05\n",
        "≈\n",
        "1.051\n",
        "e\n",
        "0.05\n",
        " ≈1.051, meaning for each additional year of Age, the odds of the event happening increase by a factor of 1.051 (about a 5.1% increase in odds).\n",
        "\n",
        "Coefficient for Income (\n",
        "𝛽\n",
        "2\n",
        "=\n",
        "0.02\n",
        "β\n",
        "2\n",
        "​\n",
        " =0.02): For each additional unit of Income, the log-odds of the event happening increase by 0.02.\n",
        "\n",
        "Odds Ratio for Income:\n",
        "𝑒\n",
        "0.02\n",
        "≈\n",
        "1.02\n",
        "e\n",
        "0.02\n",
        " ≈1.02, meaning for each additional unit of Income, the odds of the event happening increase by a factor of 1.02 (about a 2% increase in odds).\n",
        "\n",
        "e. Interpreting in Terms of Probabilities\n",
        "The sigmoid function converts the log-odds into a probability. To make an interpretation in terms of probability, you would calculate the predicted probability for different values of the features (Age, Income) using the logistic regression model.\n",
        "\n",
        "For example, if Age = 30 and Income = 50, the log-odds would be:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "−\n",
        "4\n",
        "+\n",
        "0.05\n",
        "⋅\n",
        "30\n",
        "+\n",
        "0.02\n",
        "⋅\n",
        "50\n",
        "=\n",
        "−\n",
        "4\n",
        "+\n",
        "1.5\n",
        "+\n",
        "1\n",
        "=\n",
        "−\n",
        "1.5\n",
        "logit(P)=−4+0.05⋅30+0.02⋅50=−4+1.5+1=−1.5\n",
        "Then, the predicted probability\n",
        "𝑃\n",
        "P is:\n",
        "\n",
        "𝑃\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "−\n",
        "1.5\n",
        ")\n",
        "≈\n",
        "0.182\n",
        "P=\n",
        "1+e\n",
        "−(−1.5)\n",
        "\n",
        "1\n",
        "​\n",
        " ≈0.182\n",
        "This means that, given Age = 30 and Income = 50, the model predicts a probability of 18.2% for the event occurring (class 1).\n",
        "\n",
        "3. Summary of Interpretation Guidelines\n",
        "Sign of Coefficients:\n",
        "\n",
        "Positive coefficients increase the likelihood of the event.\n",
        "\n",
        "Negative coefficients decrease the likelihood of the event.\n",
        "\n",
        "Magnitude of Coefficients:\n",
        "\n",
        "Larger magnitude means stronger influence on the probability of the outcome.\n",
        "\n",
        "Odds Ratios:\n",
        "\n",
        "For each unit increase in a feature, the odds of the outcome change by a factor of\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " .\n",
        "\n",
        "Probabilities:\n",
        "\n",
        "Use the logistic function to convert log-odds to probabilities and understand the likelihood of the event occurring for given feature values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OOy3Ba-OpZI"
      },
      "source": [
        "                                                            PRACTICAL QUESTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIkjEKgUOvu0",
        "outputId": "6199b0f6-57a1-4a92-f622-81ad928c0f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "#Q(1)Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Logistic Regression Model Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DXQyWLqO5tK",
        "outputId": "73680688-3049-4297-aa73-cc7541da9750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model with L1 Regularization (Lasso) Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "#Q(2)Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Logistic Regression Model with L1 Regularization (Lasso) Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSaW2z47PIum",
        "outputId": "91e31a26-c619-4fb5-8c25-b125170aab5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model with L2 Regularization (Ridge) Accuracy: 100.00%\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ],
      "source": [
        "#Q(3)Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Logistic Regression Model with L2 Regularization (Ridge) Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzPt5GXsPcVx",
        "outputId": "5b09554a-8572-49a7-ad16-63b442fb48f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model with Elastic Net Regularization Accuracy: 100.00%\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.38787907  1.76874047 -2.42130014 -0.70872018]\n",
            " [ 0.07955027  0.          0.         -0.58142456]\n",
            " [-1.26018631 -1.53254765  2.59567434  2.08162278]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Q(4)Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with Elastic Net regularization\n",
        "# ElasticNet is a combination of L1 and L2 regularization, controlled by l1_ratio\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Logistic Regression Model with Elastic Net Regularization Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DPIoYTjPviw",
        "outputId": "1e93d09f-4e11-4210-aee2-bd2e13fd91ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model (One-vs-Rest) Accuracy: 100.00%\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Q(5)Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with multi_class='ovr'\n",
        "# This specifies the One-vs-Rest strategy for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Logistic Regression Model (One-vs-Rest) Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yl_X3UbP5kP",
        "outputId": "63409cc9-c335-4d8e-84e4-77683c469dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'C': 1, 'penalty': 'l1'}\n",
            "Best Model Accuracy on Test Set: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#(Q)6Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='saga', max_iter=500)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],           # Different regularization strengths\n",
        "    'penalty': ['l1', 'l2']                  # Try L1 (Lasso) and L2 (Ridge)\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best hyperparameters and model accuracy\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-hBEPgqS-rK",
        "outputId": "fc0a6de5-06b2-46aa-98dd-18f75c0623e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracies for each fold: [0.96666667 1.         0.9        0.93333333 1.        ]\n",
            "Average Accuracy: 96.00%\n"
          ]
        }
      ],
      "source": [
        "#Q(7)Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy?\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset for this example)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert it into a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df.drop(columns=['target'])  # Features\n",
        "y = df['target']  # Target\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and calculate accuracy for each fold\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Calculate the average accuracy\n",
        "average_accuracy = scores.mean()\n",
        "\n",
        "# Print the accuracies for each fold and the average accuracy\n",
        "print(\"Accuracies for each fold:\", scores)\n",
        "print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXpUfsb0VgjJ",
        "outputId": "3355b28f-bf59-40b9-99b3-68d962abb2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model Accuracy: 11.11%\n"
          ]
        }
      ],
      "source": [
        "#Q(8)Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.?\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Step 2: Load dataset\n",
        "df = pd.read_csv('BIKE DETAILS.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 3: Encode non-numeric features\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Step 4: Separate features and target\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Step 5: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train model with higher max_iter\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(9)Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy?\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),       # Range of C values (Regularization strength)\n",
        "    'penalty': ['l1', 'l2'],           # Regularization type\n",
        "    'solver': ['liblinear', 'saga']    # Solvers that support L1 regularization\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=log_reg, param_distributions=param_dist, n_iter=100,\n",
        "                                   cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score (accuracy)\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "print(\"Best cross-validation score:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOzfb7rGZaRs",
        "outputId": "126e6375-3de1-4d55-de68-936f2ea8bb4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 80 is smaller than n_iter=100. Running 80 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(0.615848211066026)}\n",
            "Best cross-validation score: 0.9619047619047618\n",
            "Test accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(10)Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy?\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Use OneVsOneClassifier to apply One-vs-One multiclass classification\n",
        "ovo_classifier = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the model\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27NlIX6tZsqN",
        "outputId": "05d5987d-ed3d-44f7-82af-0a7143e5f7fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(11)Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted: 0', 'Predicted: 1'], yticklabels=['Actual: 0', 'Actual: 1'])\n",
        "plt.title('Confusion Matrix for Binary Classification')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "qzH2hMCmaBMI",
        "outputId": "f0a710be-2399-48f2-e9eb-a669a12e31c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(Q)12Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score?\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Alternatively, you can get a comprehensive report with Precision, Recall, and F1-Score for each class\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeRxuHXqaZqP",
        "outputId": "d08b0cfc-9637-418f-9db0-a9e695e1ad07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.88\n",
            "Recall: 0.83\n",
            "F1-Score: 0.85\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       145\n",
            "           1       0.88      0.83      0.85       155\n",
            "\n",
            "    accuracy                           0.85       300\n",
            "   macro avg       0.85      0.85      0.85       300\n",
            "weighted avg       0.85      0.85      0.85       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(13)Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance?\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_clusters_per_class=1, weights=[0.1, 0.9], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class weights\n",
        "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXZnseNUalo3",
        "outputId": "e2e1ab12-69bf-40bb-e26e-b12d61a26aec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.67%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.92      0.60        25\n",
            "           1       0.99      0.89      0.94       275\n",
            "\n",
            "    accuracy                           0.90       300\n",
            "   macro avg       0.72      0.91      0.77       300\n",
            "weighted avg       0.95      0.90      0.91       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(14)Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset (You can download it or use seaborn's Titanic dataset)\n",
        "# For this example, we load a sample dataset using seaborn.\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(titanic.head())\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Fill missing 'age' with the median of the column, and 'embarked' with the mode\n",
        "# Note: Column names are case-sensitive, so 'age' and 'embarked' are used instead of 'Age' and 'Embarked'\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop rows where 'fare' or 'survived' is missing\n",
        "# Note: Column names are case-sensitive, so 'fare' and 'survived' are used instead of 'Fare' and 'Survived'\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Encode 'sex' column (Male=0, Female=1)\n",
        "# Note: Column names are case-sensitive, so 'sex' is used instead of 'Sex'\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "# Note: Column names are case-sensitive, so lowercase column names are used\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 5: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27pnD1HQcCAI",
        "outputId": "f04a971f-688b-4582-da4e-c7de6e747f67"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "Accuracy: 81.34%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       157\n",
            "           1       0.80      0.73      0.76       111\n",
            "\n",
            "    accuracy                           0.81       268\n",
            "   macro avg       0.81      0.80      0.80       268\n",
            "weighted avg       0.81      0.81      0.81       268\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-889c463df609>:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-17-889c463df609>:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(15)Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Logistic Regression WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Step 7: Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 8: Logistic Regression WITH scaling\n",
        "model_scaling = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Step 9: Compare results\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {accuracy_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgqQuk9AckF9",
        "outputId": "7bda5909-b502-44d5-9c1d-6058b6f8855f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8060\n",
            "Accuracy with scaling:    0.8134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-07cd6e1e3ef6>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-36-07cd6e1e3ef6>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(16)Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scorE?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Check Columns in Dataset\n",
        "print(titanic.columns)\n",
        "\n",
        "# If 'Age' or other columns are missing, print first few rows to debug\n",
        "print(titanic.head())\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "# If 'Age' column exists, fill missing values with the median\n",
        "if 'age' in titanic.columns:\n",
        "    titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "\n",
        "# If 'embarked' column exists, fill missing values with the mode\n",
        "if 'embarked' in titanic.columns:\n",
        "    titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop rows where 'fare' or 'survived' is missing (essential columns for model)\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True) # Changed 'Fare' and 'Survived' to 'fare' and 'survived'\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "# Encode 'sex' column (Male=0, Female=1)\n",
        "if 'sex' in titanic.columns:\n",
        "    titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "if 'embarked' in titanic.columns:\n",
        "    titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']] # Changed column names to lowercase\n",
        "y = titanic['survived'] # Changed column name to lowercase\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get predicted probabilities on the test set\n",
        "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Step 8: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 9: Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (no discrimination)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Print Classification Report\n",
        "y_pred = log_reg.predict(X_test)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ycH-hHTfd93l",
        "outputId": "1deab94c-4dcb-4275-dbc0-c3a1ff71778d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
            "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
            "       'alive', 'alone'],\n",
            "      dtype='object')\n",
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "ROC-AUC Score: 0.8798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-8d2391eea560>:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-23-8d2391eea560>:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       157\n",
            "           1       0.80      0.73      0.76       111\n",
            "\n",
            "    accuracy                           0.81       268\n",
            "   macro avg       0.81      0.80      0.80       268\n",
            "weighted avg       0.81      0.81      0.81       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(17)Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracY?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Check the column names to make sure we are using the correct names\n",
        "print(\"Columns in the dataset:\", titanic.columns)\n",
        "\n",
        "# Inspect the first few rows of the dataset to ensure the columns we need are present\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(titanic.head())\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "# Check if 'Age' exists before filling missing values\n",
        "if 'age' in titanic.columns:\n",
        "    titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "else:\n",
        "    print(\"Error: 'age' column is missing in the dataset!\")\n",
        "\n",
        "# Fill missing 'Embarked' with the mode (most frequent value)\n",
        "if 'embarked' in titanic.columns:\n",
        "    titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop rows where 'fare' or 'survived' is missing\n",
        "if 'fare' in titanic.columns and 'survived' in titanic.columns:\n",
        "    titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "else:\n",
        "    print(\"Error: 'fare' or 'survived' column is missing in the dataset!\")\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "# Encode 'sex' column (male=0, female=1)\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "if 'embarked' in titanic.columns:\n",
        "    titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression with custom regularization strength (C=0.5)\n",
        "log_reg = LogisticRegression(C=0.5, max_iter=1000, solver='lbfgs')  # Using solver 'lbfgs'\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67NE2tjTe4fD",
        "outputId": "c4cb729b-0f03-4ecd-da75-b936d6d9222a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
            "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
            "       'alive', 'alone'],\n",
            "      dtype='object')\n",
            "\n",
            "First few rows of the dataset:\n",
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "Accuracy: 0.8134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-2d4cce66bcc3>:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-26-2d4cce66bcc3>:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(18)Write a Python program to train Logistic Regression and identify important features based on model coefficients?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Fill missing 'Age' with the median\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "# Fill missing 'Embarked' with the mode\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "# Drop rows where 'Fare' or 'Survived' is missing\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Encode 'sex' column (male=0, female=1)\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 4: Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression\n",
        "log_reg = LogisticRegression(C=1, max_iter=1000, solver='lbfgs')\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions and evaluate the accuracy\n",
        "y_pred = log_reg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 8: Identify important features based on model coefficients\n",
        "coefficients = log_reg.coef_[0]\n",
        "features = X.columns\n",
        "\n",
        "# Create a DataFrame for easier visualization\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient values in descending order\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Print the sorted features by importance\n",
        "print(\"\\nImportant Features based on Model Coefficients:\")\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-57cIdUfu19",
        "outputId": "11da08eb-696a-477f-a2f4-afec319f0a9a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8134\n",
            "\n",
            "Important Features based on Model Coefficients:\n",
            "    Feature  Coefficient  Absolute Coefficient\n",
            "1       sex     1.232799              1.232799\n",
            "0    pclass    -0.759557              0.759557\n",
            "2       age    -0.423234              0.423234\n",
            "3     sibsp    -0.295115              0.295115\n",
            "6  embarked    -0.233053              0.233053\n",
            "5      fare     0.131581              0.131581\n",
            "4     parch    -0.082280              0.082280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-93d3c80d3961>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-28-93d3c80d3961>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(19)Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Fill missing 'Age' with the median\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "# Fill missing 'Embarked' with the mode\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "# Drop rows where 'Fare' or 'Survived' is missing\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Encode 'sex' column (male=0, female=1)\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 4: Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression\n",
        "log_reg = LogisticRegression(C=1, max_iter=1000, solver='lbfgs')\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions and evaluate using Cohen’s Kappa Score\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate Cohen’s Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOLW65Fsf1wZ",
        "outputId": "3d7af2c8-382d-40fb-ebad-3a1ee78bacab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.6104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-38c2806d413e>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-29-38c2806d413e>:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(20)Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatioN?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Fill missing 'Age' with the median\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "# Fill missing 'Embarked' with the mode\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "# Drop rows where 'Fare' or 'Survived' is missing\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Encode 'sex' column (male=0, female=1)\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 4: Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression\n",
        "log_reg = LogisticRegression(C=1, max_iter=1000, solver='lbfgs')\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get predicted probabilities for the positive class\n",
        "y_prob = log_reg.predict_proba(X_test)[:, 1]  # Probability for class 1 (survived)\n",
        "\n",
        "# Step 8: Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Step 9: Calculate Area Under the Precision-Recall Curve (AUC)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Step 10: Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "w3WRqiuPgJAl",
        "outputId": "1f8626af-ba88-4512-8faf-80cfeb5fed76"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-9775482a4cb9>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-30-9775482a4cb9>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(21)Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy?\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Titanic dataset using seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Fill missing 'Age' with the median\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "# Fill missing 'Embarked' with the mode\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "# Drop rows where 'Fare' or 'Survived' is missing\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 2: Encode categorical variables\n",
        "# Encode 'sex' column (male=0, female=1)\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "# Encode 'embarked' column (C=0, Q=1, S=2)\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 3: Select features and target variable\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 4: Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Define the solvers to be tested\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Step 7: Train Logistic Regression models with different solvers and evaluate accuracy\n",
        "accuracy_scores = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and evaluate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy score\n",
        "    accuracy_scores[solver] = accuracy\n",
        "\n",
        "# Step 8: Print accuracy scores for each solver\n",
        "print(\"Accuracy scores with different solvers:\")\n",
        "for solver, accuracy in accuracy_scores.items():\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGrKowdcgWMe",
        "outputId": "be4db502-d7e0-4899-c7f3-1013dbdac34f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores with different solvers:\n",
            "Solver: liblinear, Accuracy: 0.8134\n",
            "Solver: saga, Accuracy: 0.8134\n",
            "Solver: lbfgs, Accuracy: 0.8134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-2045bf110796>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-31-2045bf110796>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(22)Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)?\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# Fill missing 'age' with median\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "# Fill missing 'embarked' with mode\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "# Drop rows with missing 'fare' or 'survived'\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCG_fdbJghN2",
        "outputId": "90937b4d-7d0f-40df-cc09-e7a300c507e4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.6123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-ffc7a5febe3d>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-32-ffc7a5febe3d>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(23)Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --------- Model 1: Train on RAW data ----------\n",
        "model_raw = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy without scaling: {accuracy_raw:.4f}\")\n",
        "\n",
        "# --------- Model 2: Train on STANDARDIZED data ----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# --------- Comparison ---------\n",
        "print(\"\\n--- Comparison ---\")\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"Scaling improved the model performance!\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"Scaling reduced the model performance!\")\n",
        "else:\n",
        "    print(\"Scaling had no impact on the model performance.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp-DbYongtw0",
        "outputId": "d32205d5-7190-43ed-c6ed-7e051c5956e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8060\n",
            "Accuracy with scaling: 0.8134\n",
            "\n",
            "--- Comparison ---\n",
            "Scaling improved the model performance!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-524bc0104546>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-33-524bc0104546>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q(24)Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Use GridSearchCV to find best C\n",
        "param_grid = {'C': np.logspace(-3, 3, 10)}  # Try C values from 0.001 to 1000\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best C found: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Step 8: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy with Best C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhnQobuCg5UK",
        "outputId": "40d3f98b-850e-4a85-9c84-0e4244730a2d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C found: 0.46415888336127775\n",
            "Best Cross-Validation Accuracy: 0.7993\n",
            "Test Set Accuracy with Best C: 0.8134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-0f1d699316ac>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-34-0f1d699316ac>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(Q)25Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.?\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic.dropna(subset=['fare', 'survived'], inplace=True)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Step 4: Select features and target\n",
        "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Step 5: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Save the trained model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')  # Also save the scaler!\n",
        "\n",
        "print(\"Model and scaler saved successfully.\")\n",
        "\n",
        "# Step 9: Load the saved model\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "\n",
        "# Step 10: Make predictions with the loaded model\n",
        "# (simulate new incoming data)\n",
        "sample_data = X_test.iloc[:5]\n",
        "sample_data_scaled = loaded_scaler.transform(sample_data)\n",
        "\n",
        "predictions = loaded_model.predict(sample_data_scaled)\n",
        "\n",
        "print(\"\\nSample Predictions:\", predictions)\n",
        "print(\"Actual Labels:\", y_test.iloc[:5].values)\n",
        "\n",
        "# Optional: Evaluate full test set\n",
        "y_pred_full = loaded_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"\\nTest Set Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHnsl27vhDLv",
        "outputId": "e0ad8481-44f4-41d0-eaef-e144973b4783"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and scaler saved successfully.\n",
            "\n",
            "Sample Predictions: [0 0 0 1 1]\n",
            "Actual Labels: [1 0 0 1 1]\n",
            "\n",
            "Test Set Accuracy: 0.8134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-ef08f8960577>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-35-ef08f8960577>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53mpcFrEhQo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}